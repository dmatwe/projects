**Оценка нагрузки**
<br/>
[1. Для чего](#р1)
<br/>
[2. Пользовательский трафик](#р2)
<br/>
[3. Сетевая нагрузка](#р3)
<br/>
[4. Вычислительная нагрузка](#р4)
<br/>
[5. Нагрузка на хранилище](#р5)
<br/>
[6. Стоимость](#р6)
<br/>
[Калькулятор расчета нагрузки](https://docs.google.com/spreadsheets/d/1gPkj5037qmPXl3bx4agvglszXycnRkQXPuWUZkz2VtI/edit?usp=sharing)


**High-lvl Дизайн и выбор БД**
<br/>
[1. Определение](#д1)
<br/>
[2. Требования ACID](#д2)
<br/>
[3. Основные виды БД](#д3)
<br/>
[4. Реляционные](#д4)
<br/>
[5. Key-value](#д5)
<br/>
[6. Колоночные](#д6)
<br/>
[7. Документные](#д7)
<br/>
[8. CAP теорема](#д8)
<br/>
[9. PACELC-теорема](#д9)
<br/>
[10. Выбор подходящей БД](#д10)
<br/>

**Модульный подход к дизайну и брокеры**
<br/>
[1.  Модульный дизайн Интернет-магазин](#б1)
<br/>
[2. Зачем брокеры сообщений](#б2)
<br/>
[3. Интернет-магазин Брокеры сообщений](#б3)
<br/>
[4. Преимущества очередей сообщений](#б4)
<br/>
[5. Главные принципы работы брокеров](#б5)
<br/>
[6. RabbitMQ](#б6)
<br/>
[7. Kafka](#б7)
<br/>
[8. RabbitMQ vs Kafka](#б8)
<br/>


**Масштабирование системы**





**Сервисы**

[**Сервис уведомлений**](#с1)

[**Сервис бронирования**](#с2)

[**Сокращатель ссылок**](#с3)

[**Хостинг текстов**](#с4)

[**Автодополнение**](#с5)

[**Облачный диск**](#с6)

[**Инстаграм**](#с7)

[**Телеграм**](#с8)

[**Твиттер**](#с9)

[**Нетфликс**](#с10)

[**Поиск ресторанов**](#с11)

[**Сервис такси**](#с12)
















**Расчет нагрузки на систему**
<a name="р1"></a>

Важно оценить необходимую инфраструктуру для разворачивания и поддержки системы, а также стоимость всего оборудования и процессов.

Хватит ли у заказчика материальных ресурсов, чтобы воплотить все сформулированные пожелания и требования к системе.

***Оценка позволит скорректировать ожидания от продукта.***


**Пользовательский трафик**
<a name="р2"></a>
<br/>
Минимальный набор метрик, которые необходимо определить

1. ***MAU*** - количество активных пользователей в месяц 
2. ***DAU*** - ежедневное количество активных пользователей 
3. общее количество пользователей на каком-то горизонте (например, 5 лет)
4. Сколько какого контента в среднем производит типичный пользователь в день

***Отталкиваясь от входных бизнес-показателей мы сможем перейти к оценкам
нагрузки:***

1. ***RPS*** - какое максимальное число запросов в секунду будет к частям системы
2. Какое количество одновременных соединений нужно будет удерживать
3. Какой будет нагрузка на сетевой канал и какой общий трафик мы потратим
4. Какое будет приращение занимаемого места и сколько данных накопится
5. Во-сколько нам обойдутся трафик и железо, которые смогут это все поддерживать


**Сетевая нагрузка**
<a name="р3"></a>
<br/>
Cостоит из удерживаемых соединений в единицу времени и общего объёма трафика, который может пройти через сеть за единицу времени.

***Ограничения по количеству удерживаемых соединений***

Современный сервер может выдержать наплыв в 10 тысяч — 100 тысяч соединений. Если по вашим расчётам количество пользователей выйдет за этот диапазон, следует заложиться на расширение серверных мощностей.



***Ограничения по трафику***

Максимальный предел трафика упирается в пропускную способность выбранной технологии и канала связи.

Вот некоторые данные по пропускной способности:

***Облако*** — до 1GbE в секунду;

***Витая пара*** — до 10GbE в секунду;

***Оптоволокно стандарта QSFP+*** - до 40 GbE;

***Infiniband (IB) — 100+ GB.***  Этот вид соединений используется в суперкомпьютерах и в огромных дата-центрах (очень дорогой вариант).

Также можно вывести общую стоимость трафика. Облачные провайдеры, например, оценивают 1GB трафика от 1 до 10 центов. Таким образом, зная общий трафик за месяц можно оценить стоимость месячного трафика необходимого для функционирования сервиса.


**Вычислительная нагрузка**
<a name="р4"></a>

<br/>
Под вычислительной нагрузкой обычно подразумевается вычисление количества одновременных запросов пользователей к сервису. Этот показатель исчисляется в RPS (requests per second).

По грубой оценке можно исходить из того, что в простых сценариях ***в облаке** мы сможем выдерживать нагрузку в 
<br/>
***100k RPS*** для текстовых данных
<br/>
***10k RPS*** при чтении в БД
<br/>
***1k RPS*** при записи в БД.

При наличии мощных ***физических серверов*** в тех же сценариях можно прикидывать вероятную нагрузку в 
<br/>
***500k RPS*** для текстовых данных
<br/>
***50k RPS*** при чтении в БД
<br/>
***5k RPS*** при записи в БД.


**Нагрузка на хранилище**
<a name="р5"></a>
<br/>
Оценив количество создаваемого контента со стороны пользователей, мы можем прикинуть, сколько места он будет занимать, какой постоянный прирост при этом ожидается и, соответственно, сколько того или иного железа нам потребуется для нашей системы.


***HDD*** со скоростью 100-300 МБ/с, каждый диск хранит до 20 ТБ по цене до $500;

***NVMe SSD*** со скоростью 3-5 ГБ/с, каждый диск хранит до 8 TБ по цене до $2000;

***Оперативная память (RAM)*** со скоростью 50 ГБ/с, стоимость планки 128 ГБ около $1000.


В среднем хранение 1 ТБ обойдётся в около $10000 в RAM, $300 на SSD и $30 на HDD.

Можно прикидывать, что в одном сервере будет до 1 ТБ RAM, 50 ТБ SSD или 200 ТБ HDD.

Если дисков много, в игру вступает и среднее количество отказов за год (AFR) в размере 1%.

[Калькулятор расчета нагрузки](https://docs.google.com/spreadsheets/d/1gPkj5037qmPXl3bx4agvglszXycnRkQXPuWUZkz2VtI/edit?usp=sharing)






**Выводы**
<a name="р6"></a>
<br/>
На какие показатели стоит ориентироваться при расчётах нагрузки на систему:

1. Трафик: MAU, DAU, прирост контента в день, всего за 5 лет, соотношение R/W;
2. Сеть: С10k давно не вопрос, 1 Gb p/s даже в облаке, платим не более $0.1/GB;
3. Вычисления: отдаём 100к текста, считываем 10к и записываем 1к простых запросов в БД;
4. Хранилище: HDD 300 MB/s и $30/TB, SSD 5 GB/s и $300/TB, RAM 50GB/s и $10000/TB;
5. В одну машину при этом можно поставить до 1 TB RAM, 50 TB SSD или 200 TB HDD.


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/mbgbtb.jpeg)

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/mbps.jpeg)



**Высокоуровневый дизайн**
<a name="д1"></a>

Под высокоуровневым дизайном мы будем понимать такой дизайн, который минимально учитывает основные сценарии взаимодействия с системой, но при этом не отражает всю сложность взаимодействия между большим количеством реальных компонентов и не принимает во внимание ту нагрузку, под которой система будет в продуктивной среде.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/design.png)

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/design2.png)


***Общий дизайн***
<br/>
В голове можно всегда держать общий дизайн, подходящий почти для всего:

1. Часть системы взаимодействует непосредственно с юзером (какой-то front-end);
2. Часть системы работает скрыто от пользователя под капотом (какой-то back-end);
3. Часть системы заточена на долгосрочное хранение данных (базы данных, файловые системы и т.п.).


**Требования ACID — это набор требований, которые обеспечивают сохранность ваших данных.**
<a name="д2"></a>


1. ***Atomicity — Атомарность.*** Гарантирует, что каждая транзакция будет выполнена полностью или не будет выполнена совсем. Не допускаются промежуточные состояния.
2. ***Consistency — Согласованность.*** Это свойство вытекает из предыдущего. Благодаря тому, что транзакция не допускает промежуточных результатов, база остается консистентной. Есть такое определение транзакции: «Упорядоченное множество операций, переводящих базу данных из одного согласованного состояния в другое». То есть до выполнения операции и после база остается консистентной.
3. ***Isolation — Изолированность.*** Во время выполнения транзакции параллельные транзакции не должны оказывать влияния на её результат.
4. ***Durability — Надёжность.*** Если пользователь получил подтверждение от системы, что транзакция выполнена, он может быть уверен, что сделанные им изменения не будут отменены из-за какого-либо сбоя. Обесточилась система, произошел сбой в оборудовании? На выполненную транзакцию это не повлияет.

**Основные виды БД**
<a name="д3"></a>

1. ***Реляционные*** — базы данных, записи в которых хранятся в виде набора таблиц и заранее определённых связей между ними. К таким БД относятся все СУБД, а также базы удовлетворяющее ACID-требованиям.
2. ***БД типа key-value*** — поддерживают и хранят только наиболее простые операции с записями по ключу.
3. ***Колоночные БД*** — данные хранятся в виде набора колонок, не связанных между собой. Каждая запись, которую нужно сохранить в базу, разбивается на части, каждая часть записывается в свою колонку.
4. ***Документные БД*** — хранят данные в виде сложных древовидных структур типа JSON.
5. ***Графовые базы данных*** - вершины, ребра и их свойства.
![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/graf.png)



**Реляционные базы данных**
<a name="д4"></a>

Реляционные БД — базы данных, основанные на реляционной модели, подразумевающей организацию данных в виде таблиц (строк-записей и столбцов-атрибутов), каждая запись в которой имеет уникальный ключ и может в атрибутах ссылаться на ключи в других таблицах. Можно считать это «базой данных по умолчанию», т.к. при создании MVP вашего проекта вы наверняка подключите БД вроде SQLite, Postgres, MySQL и т.п. для хранения ваших записей.
<br/>
Все реляционные базы данных соблюдают ACID-требования.



**БД вида key-value**
<a name="д5"></a>

База данных на основе пар «ключ‑значение» — это тип нереляционных баз данных, в котором для хранения данных используется простой метод «ключ‑значение». База данных на основе пар «ключ‑значение» хранит данные как совокупность пар «ключ‑значение», в которых ключ служит уникальным идентификатором. Как ключи, так и значения могут представлять собой что угодно: от простых до сложных составных объектов.

Базы данных с использованием пар «ключ‑значение» поддерживают высокую разделяемость и обеспечивают беспрецедентное горизонтальное масштабирование, недостижимое при использовании других типов баз данных. Например, Amazon DynamoDB выделяет дополнительные разделы на таблицу, если существующий раздел заполняется до предела и требуется больше пространства для хранения.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/key.png)

***Примеры использования*** 

***Хранилище сессий Redis***

Основанное на сессиях приложение (например, интернет‑приложение) запускает сессию, когда пользователь входит в систему. Оно активно до тех пор, пока пользователь не выйдет из системы или не истечет время сессии. 
<br/>
В течение этого периода приложение хранит все связанные с сессией данные либо в основной памяти, либо в базе данных. 
<br/>
Данные сессии могут включать информацию профиля пользователя, сообщения, индивидуальные данные и темы, рекомендации, таргетированные рекламные кампании и скидки.
<br/>
Каждая сессия пользователя имеет уникальный идентификатор. Данные сессий всегда запрашиваются только по первичному ключу, поэтому для их хранения отлично подходит быстрое хранилище пар «ключ‑значение». 
<br/>
В целом базы данных на основе пар «ключ‑значение» могут снижать накладные расходы в расчёте на страницу по сравнению с реляционными базами данных.


***Корзина интернет‑магазина MemcacheDB***

Во время праздничного сезона покупок сайт интернет‑магазина может получать миллиарды заказов за считанные секунды. 
<br/>
Используя базы данных на основе пар «ключ‑значение», можно обеспечить необходимое масштабирование при существенном увеличении объемов данных и чрезвычайно интенсивных изменениях состояния. 
<br/>
Такие базы данных позволяют одновременно обслуживать миллионы пользователей благодаря распределённым обработке и хранению данных. 
<br/>
Базы данных на основе пар «ключ‑значение» также обладают встроенной избыточностью, что позволяет справляться с потерей узлов хранилища.


***Колоночные базы данных***
<a name="д6"></a>

Колоночные БД призваны решить проблему неэффективной работы традиционных СУБД в аналитических системах и системах c подавляющим большинством операций типа «чтение». 
<br/>
Они позволяют на более дешёвом и маломощном оборудовании получить прирост скорости выполнения запросов в 5, 10 и иногда даже в 100 раз, при этом, благодаря компрессии, данные будут занимать на диске в 5-10 раз меньше, чем в случае с традиционными БД.

У колоночных БД есть и недостатки — они медленно работают на запись, не подходят для транзакционных систем и, как правило, ввиду «молодости» имеют ряд ограничений для разработчика, привыкшего к развитым традиционным БД.

Обычно колоночные БД применяются в аналитических системах класса business intelligence (ROLAP) и аналитических хранилищах данных (data warehouses). Причём объёмы данных могут быть достаточно большими — есть примеры по 300-500ТБ и даже случаи с >1ПБ данных.

Примеры БД:
<br/>
Hbase
<br/>
Vertica (работает с VPN)
<br/>
ClickHouse

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/column.png)


***Документные базы данных***
<a name="д7"></a>

Документная база данных — это тип нереляционных баз данных, предназначенный для хранения и запроса данных в виде документов в формате, подобном JSON.

Документные базы данных позволяют разработчикам хранить и запрашивать данные в БД с помощью той же документной модели, которую они используют в коде приложения. Гибкий, полуструктурированный, иерархический характер документов и документных баз данных позволяет им развиваться в соответствии с потребностями приложений.

Документная модель хорошо работает в таких примерах использования, как каталоги, пользовательские профили и системы управления контентом, где каждый документ уникален и изменяется со временем. Документные базы данных обеспечивают гибкость индексации, производительность выполнения стандартных запросов и аналитику наборов документов.

В следующем примере документ в формате, подобном JSON, описывает книгу.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/doc.png)


***Примеры использования***

***Управление контентом***

Документная база данных – отличный выбор для приложений управления контентом, таких как платформы для блогов и размещения видео. При использовании документной базы данных каждая сущность, отслеживаемая приложением, может храниться как отдельный документ.

Такой вид базы данных позволяет разработчику с удобством обновлять приложение при изменении требований. Кроме того, если необходимо изменить модель данных, то требуется обновление только затронутых этим изменением документов. Для внесения изменений нет необходимости обновлять схему и прерывать работу базы данных.


***Каталоги***

Документные базы данных эффективны для хранения каталожной информации. 
<br/>
Например, в приложениях для интернет‑коммерции разные товары обычно имеют различное количество атрибутов.
<br/>
Управление тысячами атрибутов в реляционных базах данных неэффективно. Кроме того, количество атрибутов влияет на производительность чтения. 
<br/>
При использовании документной базы данных атрибуты каждого товара можно описать в одном документе, что упрощает управление и повышает скорость чтения. 
<br/>
Изменение атрибутов одного товара не повлияет на другие товары.

Из примеров:
<br/>
MongoDB
<br/>
CouchDB
<br/>
Amazon DocumentDB



***CAP теорема***
<a name="д8"></a>


В CAP говорится, что в распределённой системе возможно выбрать только два из трёх свойств:

1. C (consistency) — согласованность. Каждое чтение даст вам самую последнюю запись.
2. A (availability) — доступность. Каждый узел (не упавший) всегда успешно выполняет запросы (на чтение и запись).
3. P (partition tolerance) — устойчивость к распределению. Даже если между узлами нет связи, они продолжают работать независимо друг от друга.

***PACELC-теорема***
<a name="д9"></a>


Вся теорема сводится к if P -> (A or C), else (L or C), или же:

В случае наличия разделения (P) надо выбирать между доступностью (A) и согласованностью (C).

Иначе (Else) мы можем дополнительно выбирать между меньшей задержкой (L) и согласованностью (C).

Latency — это время, за которое клиент получит ответ и которое регулируется каким-либо уровнем consistency. Latency в некотором смысле представляет собой степень доступности.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/PACELC.png)


***Выбор подходящей БД***
<a name="д10"></a>


При проектировании подсистемы требуется определиться, что из C, A и P нам важнее:

***У согласованных и доступных систем (CA)*** есть сложности с устойчивостью к разделению, которые частично решается созданием реплик, а также такие системы масштабируют вертикально (мощнее железо), чтобы изначально избежать разделения. Именно сюда попадают RDBMS с ACID, которые всегда кстати для хранения чувствительных данных.

***В согласованных и устойчивых к разделению системах (CP)*** могут быть проблемы с доступностью, на зато данные в разделённых нодах всегда корректны и выглядят одинаково. Примеры таких БД есть в разных семействах — колоночная HBase, документная Mongo и Redis.

***В доступных и устойчивых к разделению системах (AP)*** нет строгой согласованности, но она так или иначе достигается, чего бывает достаточно для крупных систем (например, чей-то новый пост на другом конце света увидят на минуту позже). 
<br/>
Примеры опять же есть разные — колоночная Cassandra, документная CouchDB и Dynamo. В противовес к строгим ACID выделяют BASE (basic availability, soft state, eventual consistency).

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/bdchose.png)

Документные БД - для хранения атрибутов товаров 

Реляционные БД - для хранение баланса монет у игроков

Колоночные БД - для хранение истории кликов на сайте

Key-value БД - для хранения статусов пользователей


**Модульный подход к дизайну**
<a name="б1"></a>

Подсервис покупки логически должен далее обратиться в подсервис, связанный отправкой на доставку.

После совершения покупки логично отправить какое-то уведомление пользователю.
<br/>
Сервис отправки уведомлений взаимодействует с БД пользователей, чтобы узнать почту и предпочтения по уведомлениям

После совершения покупки нам также необходимо обновить информацию о наличии тех или иных товаров.

Помимо простых покупателей обновлять инвентарь и работать с заказами могут и продавцы. 
<br/>
Им предоставляется панель администрирования, где можно создавать новые товары, обновлять количества, делать выгрузки в csv файлы по различным статистикам.

Рассылкой писем могут заниматься и люди из отдела маркетинга через соответствующие инструменты. Здесь будет использоваться тот же подсервис отправки уведомлений почтой, что и для рассылок по подтверждению заказов. 


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/shop.png)

***Плюсы:***

1. Мы сразу отображаем картину того, какие смысловые части приложения должны быть реализованы;
2. Легко переносить функциональные требования в дизайн, на каждое требование можно выделить модуль;
3. У разных модулей разная нагрузка, масштабировать их можно независимо (но будут нужны балансировщики).

***Минусы:***

1. Со временем зависимости могут расти;
2. Неравномерная нагрузка (пользователи чаще совершают покупки, чем продавец влияет на заказ).


***Сложности:***

1. Уже на рассмотренном примере становится очевидна сложность образующихся связей между частями системы;
2. Продолжение работы зависимого сервиса в принципе зависит от работоспособности используемого;
3. Зависимый сервис начнет тормозить в случае наплыва обращений в используемом сервисе (вероятно, нужно создавать несколько инстансов, использовать локдаун и тому подобное — поговорим чуть позднее).

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hard.png)



**Очереди сообщений**
<a name="б2"></a>

***Очередь сообщений*** — сущность, в которую будут приходить сообщения от разных сервисов, которые называются продьюсерами (создают новые сообщения).
Далее сообщения будут использоваться в других сервисах, которые называются консьюмерами (получают сообщения и осуществляют какую-то работу над ними).

***Интернет-магазин***
<a name="б3"></a>

Вернёмся к примеру: после совершения заказа и оплаты придёт сообщение о том, что товар куплен. Оно попадает в очередь сообщений. Далее сервис, отвечающий за доставку:

либо сразу получает такие сообщения;
<br/>
либо приходит и забирает накопившиеся сообщения.

Соответственно, очередь сообщений используется внутри какого-то брокера сообщений, отвечающего за взаимодействие с очередью.


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/broker.png)

***Преимущества очередей сообщений***
<a name="б4"></a>

1. Сервисы разъединяются на логические составляющие и не используют друг друга напрямую;
2. Разъединённые сервисы можно независимо и просто масштабировать за счёт количества воркеров;
3. Очереди лежат на отдельном инстансе и основные свободны от лишней работы, косвенно ускорены.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/broker1.png)

Например, сервис заказов генерирует 100 RPS, а один консьюмер доставки только 10 RPS, тогда можно сделать 10 воркеров. Если появляется нужда обрабатывать больше сообщений (например, отмены заказов продавцами), можно добавить ещё необходимое количество воркеров.


***Устройство очереди сообщений***
<a name="б5"></a>

1. Одни клиентские приложения (продьюсеры) создают и отправляют сообщения брокеру (т.е. кладут в очередь);
2. Другие же приложения (консьюмеры) подключаются к очереди и подписываются на сообщения для обработки;
3. Приложения могут быть как продьюсерами, так и консьюмерами, так и тем и другим одновременно. Сообщения лежат в очереди до тех пор, пока какой-либо консьюмер не получит их.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/broker2.png)

***Сценарии взаимодействия:***

1. push based подход — брокер получает сообщение и тут же посылает его в какой-то консьюмер, подписанный на данный вид сообщений;

2. pull based подход — сообщения, посылаемые брокеру, складываются, хранятся некоторое время, а потом приходит консьюмер и забирает нужное количество сообщений. 
<br/>
Например, вы не успели отмасштабировать сервис, пришла запредельная нагрузка, сообщения будут скапливаться в очереди и обрабатываться чуть позднее. 
<br/>
В такие часы пик можно постфактум добавить больше консьюмеров, а потом их убрать.

***Сценарии применения***

Один сервис использует функционал другого сервиса, возможно нескольких других;
<br/>
Нет необходимости получать финальный ответ, но при этом надо продолжать работу.

Пример: пользователь загружает видео, а затем в фоне оно перекодируется и создаются превью.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/broker3.png)

**RabbitMQ**

В реальности сервисы скорее всего общаются с каким-то брокером сообщений, который внутри отвечает за взаимодействие с очередью.

***Устройство***

1. Продьюсеры отправляют сообщения не в очередь напрямую, а в обменник (exchange);
2. Обменник в свою очередь отвечает за маршрутизацию сообщений по разным очередям;
3. Сообщения маршрутизируются на основе привязок (binding, подобие доменного адреса) и ключей маршрутизации (routing keys).


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/rabbit.png)

***Типы используемых обменников***

1. Direct: сообщение отправляется в очередь, ключ которой совпадает с ключом привязки;
2. Topic: сообщения отправляются в очереди, ключ которых удовлетворяет шаблону;
3. Fanout: все сообщения отправляются во все привязанные очереди;
4. Header: маршрутизация осуществляется на основе заголовков в сообщениях.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/rabbit2.png)


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/rabbit3.png)


**Apache Kafka**

***Устройство***

1. Приложения может подключаться к системе и добавлять записи (records) в создаваемые топики (topics);
2. Другие приложения могут подключаться, чтобы обрабатывать накопленные записи в топиках;
3. Присланные данные сохраняются в течение заданного в системе срока (может даже и месяца);
4. Записи обязательно имеют ключ и значение (для ключа хранится последнее значение), а наличие временных меток и заголовков опционально.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/kafka.png)


5. Кластер в Кафке состоит из одного или нескольких серверов (брокеров Кафки);
6. Продьюсеры присылают (push) записи в топики в рамках брокера;
7. Консьмеры забирают (pull) записи из выбранного топика;
8. Можно работать и с одним брокером, но кластер даёт больше возможностей (например, репликацию);
9. Брокеры управляются отдельными приложениями в рамках системы — Zookeeper.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/kafka1.png)

***Топики***

***Топик*** — это название категории/фида, в котором публикуются и хранятся записи. Все запись разделены по топикам;

***Продьюсеры*** пишут в топики, консьюмеры забирают записи из них. Записи хранятся в течение настроенного срока;

***Кафка*** фактически хранит весь лог из записей и это задача консьюмеров — следить за отступами (offset) в топиках

Обычно ***консьюмеры*** просто сдвигают отступ по мере прочтения записей, но могут и откатить при необходимости

***Кафка-топики*** разбиваются на определённое количество партиций, которые хранят записи последовательно;

***Топики*** тем самым могут быть распараллелены по нескольким брокерам;

***Репликация*** также работает на уровне партиций, у каждой партиции есть реплика-лидер и её последователи;

***Продьюсеры*** присылают записи лидеру, он коммитит записи в лог и только потом они становятся видны консьюмерам;

***Продьюсеры*** сами определяют, в какую партицию им нужно писать, для этого можно добавить ключ к записи (или хэш).


***Низкоуровневые консьюмеры***

1. Сами должны выбрать топик и партицию, а также отступ, с которого начинается чтение;
2. В качестве отступа можно выбрать как конкретную позицию, так и начало или конец топика.

***Высокоуровневые консьюмеры***

1. Группа консьюмеров, состоящая из одного или нескольких консьюмеров с одним свойством «group.id»;
2. Брокер распределяет и перераспределяет партиции между консьюмерами из одной группы;
3. Брокер также следит за оффсетами группы во всех партициях;
4. Количество консьюмеров в группе может быть большим, но не больше числа партиций;
5. Консьюмеры всегда сами запрашивают записи, когда они готовы к их обработке;
6. Все записи хранятся в Кафке, поэтому у консьюмеров не случается перегрузов, они всегда могут «нагнать» топик.


***Записи***

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/kafka2.png)

6 партиций в примере на рисунке

Первое сообщение в первую партицию первого топика записывается с оффсетом 0

Дальнейшие сообщения в ту же партицию будут иметь оффсеты 1, 2, 3 и т.д. (записи хранятся в порядке поступления)



***Пример приложения***

Собираем пользовательские логи, включающие данные о просмотрах и кликах по баннерам.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/kafka3.png)

***Об одном полезном свойстве***
Для каждого конкретного ключа хранится только последняя запись с этим ключом, остальные удаляются.
<br/>
Таким образом происходит оптимизация хранимого лога событий внутри Kafka. 
<br/>
Консьюмеры начинают чтение с последнего значения по выбранному ключу и затем получают обновления.

Это можно использовать для восстановление кэша или получения разных статусов из источника правды в виде Кафки.


**RabbitMQ vs Kafka**
 
***Для чего лучше подходит Kafka***

1. Лучше всего для стриминга из A в B без сложной маршрутизации, но с максимальной производительностью;
2. Идеально для логирования событий, обработки потока и отслеживания последовательных изменений в системе;
3. Подходит для обработки данных в многоуровневых пайплайнах.
4. Кратко: используем для хранения и последующей, возможно повторной, обработки и анализа потоковых данных.

***Для чего лучше подходит RabbitMQ***

1. Применяется для тяжёлых фоновых задач, а также коммуникации внутри сложных приложений;
2. Идеально для веб-сервисов с низкими задержками благодаря делегированию нагрузки множеству воркеров;
3. Также подходит для фоновых и долгоиграющих задач вроде конвертации видео или обработки изображений.
4. Кратко: используем с долгими фоновыми задачами и для общения и интеграции между различными сервисами.



**Масштабирование системы**

***Балансировщик нагрузки (load balancer)*** — критически важный компонент любой распределённой системы, который распределяет передаваемый трафик между разными приложениями, сервисами и хранилищами.

***Балансировщик*** также может следить за статусом сервисов, в которые он передаёт запросы, чтобы перестать передавать запросы к серверам, переставшим отвечать в течение заданного времени.

Естественной задачей для балансировщика является балансировка пользовательского трафика — распределение приходящих запросов от пользователя между инстансами сервиса:

***Цель балансировки нагрузки*** — оптимизация использования ресурсов, максимизация пропускной способности, уменьшение времени отклика и предотвращение перегрузки какого-либо одного ресурса.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/balancer.png)


***Принцип работы балансировщика нагрузки***

1. Анализ того, какие из серверов готовы принять и обработать запрос пользователя;
2. Выбор наиболее подходящего сервера по заданному алгоритму.

***Алгоритм выбора подходящего сервера***

1. По количеству подключений — приоритет серверу с минимальным количеством подключений;
2. По времени ответа — приоритет наименьшей задержке перед ответом;
3. По трафику — приоритет серверу с минимальной сетевой нагрузкой;
4. Round-robin — на каждый запрос меняем сервер по кругу из заранее определённой очереди;
5. Weighted round-robin — серверы получают веса на основе комбинации из характеристик выше;
6. Пользовательский хэш — выбираем сервер на основе хэша (IP, user_id, ...).

***Преимущества использования балансировщика***

1. Улучшается пользовательский опыт за счёт передачи запроса подходящему серверу;
2. Снижается нагрузка на каждый из серверов за счёт распределения запросов между ними;
3. Справляемся с отказом части серверов, по крайней мере, в случае stateless сервиса;
4. Умные балансировщики могут даже предсказывать рост нагрузки и инициировать автоскейлинг.



***Распределение данных***

***Секционирование (partitioning, партиционирование)*** — разделение больших хранимых файлов или объектов в базах данных, таких как таблицы, индексы и пр. на логические части по каким-то критериям. Секционирование повышает масштабируемость и производительность системы;

***Избыточность и репликация.*** Под избыточностью понимается дублирование важных элементов системы для повышения надежности. Под репликацией понимается копирование данных в избыточные сервисы для консистентности.


***Секционирование***

Выделяют несколько видов секционирования:

1. Вертикальное секционирование (по умолчанию) подразумевает выделение разных смысловых частей данных и последующее вероятное разнесение их по разным машинам, запас тем самым ограничен;

2. Горизонтальное секционирование, оно же шардирование (sharding) подразумевает разбиение данных на уровне таблиц по выбранному ключу, например, дню недели или почтовому индексу.

***Способы секционирования записей***

1. ***На основе ключа или хеша:*** применяем хеш-функцию к какому-либо из полей и получаем номер партиции. Например, если мы располагаем сотней серверов и сохраняем время создания записей как int, то номер партиции можно определить по формуле «timestamp % 100». При удачном выборе такого ключа (не weekday % 7) данные будут распределены предопределённым образом и равномерно. Из минусов — фиксированное количество серверов. Выпадет один и начнется перекос в нагрузке;
2. ***По спискам*** — мы можем заранее распределить все поступающие записи по предопределённым группам на основе страны, дня недели и т.п., понимая, как соотносится нагрузка из разных групп;
3. ***Round-robin разбиение*** — меняем партиции по кругу с каждой новой поступающей записью;
4. ***Составное разбиение*** — комбинируем что-либо из вышеупомянутого.


***Недостатки метода***

1. ***Выполнение JOIN-ов*** — они теперь затрагивают данные на нескольких серверах и часто невыполнимы. Частично решается денормализацией БД;
Большинство РСУБД не могут следить за валидностью внешних ключей на разных серверах;
2. ***Ребалансировка*** — иногда нам нужно изменить способ шардирования, если поздно поняли уязвимость текущего способа, и как следствие получили неравномерность данных или запросов по серверам. Скорее всего это приведёт к пересылке всех данных между серверами и недоступности все это время.

***Избыточность***

Резервный сервис в пару к действующему на отдельной машине

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/izb.png)


Размноженные копии файлов с заданным коэффициентом репликации.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/izb2.png)


***Репликация***

Репликация повышает надёжность, устойчивость к отказам и доступность системы.

При репликации баз данных у нас есть множество разных копий одних и тех же данных.


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/rep.png)

***Как гарантировать их консистентность для всех пользователей:***

1. Кворум

Некий состоящий из ⌊N/2⌋ + 1 узлов уровень согласованности носит название кворума, большинства узлов. В случае разбиения сетевой среды на части или отказа узлов, в системе с 2f + 1 узлами, живые узлы способны продолжать приём чтений и записей при недоступности до f узлов до тех пор, пока оставшаяся часть кластера не станет снова доступной. Иными словами, такая система безразлична к отказу не более f узлов.

Выполняя операцию чтения и записи с применением кворума, система не может быть безразлична к отказу большинства узлов. Например, когда в сумме имеются три реплики, а две из них упали, операции чтения и записи не способны достичь значения числа узлов, требуемого для согласованности чтения и записи, ибо лишь один узел из трёх способен отвечать на соответствующие запросы.

Достижение кворума как критерий успешности операции гарантирует консистентность данных. Тем не менее необходимость достижения кворума влечет и эпизодическую недоступность сервиса.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/kvrm.png)

2. Лидер и последователи

В каждый момент времени есть только один лидер, который осуществляет репликацию данных. Последователи пишут данные по запросу лидера, служат резервной копией и помогают при чтении. В случае отказа лидера один из последователей становится новым лидером:

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/rep1.png)


***Консистентное (согласованное) хеширование***

При использовании стандартных методов распределения данных между серверами может возникнуть ситуация, когда один или несколько серверов становятся недоступны. Тогда приходится перестраивать логику распределения данных. Такой подход очень ресурсоёмкий и не позволяет добиться хороших показателей по скорости ответа системы. С данной проблемой может помочь подход, называемый согласованным или консистентным хешированием.

Это особый вид хеширования, отличающийся тем, что когда хеш-таблица перестраивается, только K/n ключей в среднем должны быть переназначены, 
<br/>
где K — число ключей 
<br/>
n число слотов (slots, buckets). 
<br/>
В противоположность этому, в большинстве традиционных хеш-таблиц, изменение количества слотов вызывает переназначение почти всех ключей.


***Суть алгоритма заключается в следующем:***

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hash.png)

мы рассматриваем набор целых чисел от 0 до 2^32,
<br/>
«закручивая» числовую ось в кольцо (склеиваем 0 и 2^32).
<br/>
Каждому серверу из пула серверов мы сопоставляем число на кольце (рисунок слева, сервера A, B и C). 
<br/>
Ключ хешируется в число в том же диапазоне (на рисунке – синие точки 1-4), 
<br/>
в качестве сервера для хранения ключа мы выбираем сервер в точке, ближайшей к точке ключа в направлении по часовой стрелке. 
<br/>
Если сервер удаляется из пула или добавляется в пул, на оси появляется или исчезает точка сервера,
<br/>
в результате чего лишь часть ключей перемещается на другой сервер.

На рисунке 2 справа показана ситуация, когда сервер C был удалён из пула серверов и добавлен новый сервер D. 
<br/>
Легко заметить, что ключи 1 и 2 не поменяли привязки к серверам, а ключи 3 и 4 переместились на другие сервера. 
<br/>
На самом деле одному серверу ставится в соответствие 100-200 точек на оси (пропорционально его весу в пуле), что улучшает равномерность распределения ключей по серверам в случае изменения их конфигурации.

В случае консистентного хеширования ключей ранее рассмотренный пример будет выглядеть так, что ключи, которые были на 3х серверах и остались в рабочем состоянии, на них же и останутся. А ключи с 4-го сервера равномерно распределятся по 3-м оставшимся, мы потеряли ровно 25% ключей — возможный минимум.




**Сервис соц сети**

Представим, что у нас есть простой сервис, который нужно подготовить к масштабированию. Из функционала есть редактирование профиля, заливка медиа и получение готовых рекомендаций.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/netw.png)

Как можно масштабировать такой сервис:

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/netw2.png)

1. Приходит много пользователей, и их запросы проходят через loadbalancer. Также существует его реплика, которая находится в режиме ожидания и будет работать в случае поломки основного балансировщика;
2. Балансировщик нагрузки выбирает, какому сервису передать входящий запрос;
3. Также у приложения есть много разных инстансов, которые должны отрабатывать запросы за адекватное время. Если один из них перестаёт отвечать, то опционально мы передаём информацию в автоскейлер, который создаст новый инстанс;
4. Дополнительные балансировщики нагрузки и их копии для сервисов. У каждого сервиса также есть инстансы;
У баз данных есть реплики, которые помогут сервису продолжать работу в случае неполадок с оригинальной базой данных.

***Выводы***

1. При масштабировании и увеличении трафика появляется необходимость в балансировщике нагрузки;
2. При масштабировании и увеличении трафика появляется необходимость секционировать данные;
3. Для повышения надежности можно добавлять избыточные сервисы и реплицировать данные;
4. Есть разные парадигмы для синхронизации реплик, такие как кворум или лидер и последователи;
5. При шардировании нужно выбирать ключ так, чтобы был баланс и в нагрузке, и в занимаемом месте;
6. При генерации ключа для выбора шарда стоит прибегать к консистентному хешированию.

**Сервис уведомлений**

***Зачем нужен такой сервис?***
<br/>
Чтобы приложения могли доставить нам ну очень важную информацию даже на утюг или холодильник.


***Какие сценарии отобразим?***

Партнёры могут делать рассылки либо через интерфейс, либо через API;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/not.png)

Рассылки должны производиться на какие только возможно устройства пользователя;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/not1.png)

Информация обо всех рассылках должна сохраняться на будущее на всякий случай;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/not2.png)


Добавим очередь сообщений: разъединяем зависимые сервисы, выделяем естественные фоновые процессы.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/not3.png)


**Сервис бронирования**

***Зачем нужен такой сервис?***

Чтобы было где пережидать периоды структурных трансформаций и поисков новых моделей бизнеса.

***Какие сценарии отобразим?***

Владельцы отелей регистрируются и пополняют жилой фонд;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hotel.png)

Пользователи регистрируются, могут искать и бронировать жильё;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hotel2.png)

И владельцы, и пользователи могут редактировать бронь.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hotel3.png)

После завершения брони её можно положить в архив;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hotel4.png)

При изменениях в брони уведомляем пользователя;

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hotel5.png)

Добавляем очередь сообщений: отсоединяем добавленные в качестве приправы сервисы и общаемся с ними через очередь.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/hotel6.png)


**Сокращатель ссылок**

***Зачем нужен данный сервис?***
<br/>
Чтобы превращать длинные ссылки в короткие и делиться ими.

***Функциональные требования***


1. По полной ссылке пользователь может сгенерировать короткую;
2. Когда пользователь переходит по короткой ссылке, он перенаправляется на исходную;
3. У пользователей есть возможность самим выбрать сокращённый синоним;
4. Ссылки «протухают» через какое-то время. Пользователи могут его изменять;
5. Cбор аналитики по переходам.



***Нефункциональные требования***


1. Высокая доступность сервиса — иначе ссылки просто перестают работать;
2. Перенаправление должно происходит с минимальной задержкой;
3. Короткие ссылки должны быть случайны настолько, чтобы их нельзя было подобрать.
4. Предоставление API для разработчиков.

| Вводные данные              | Значение     | Ед. измерения |
| --------------------------- | ------------ | ------------- |
| DAU<br>Daily active users   | 1 000 000,00 |               |
| INSERT<br>Записи на вставку | 3            |               |
| READ<br>Чтение записей      | 100          |               |
| WEIGHT<br>Вес одной записи  | 1            | KB            |

| Вычисления                                                    |         |
| ------------------------------------------------------------- | ------- |
| Вычислительная нагрузка                                       |         |
| RPS INSERT<br>Формула: (DAU X INSERT) / Кол-во сек в дне      | 34,72   |
| RPS READ<br>Формула: (DAU X INSERT X READ) / Кол-во сек в дне | 3472,22 |

| Cеть / Трафик                                       |        |        |       |       |       |
| --------------------------------------------------- | ------ | ------ | ----- | ----- | ----- |
| Ед. измерения                                       | KB     | Mbps   | MBps  | Gbps  | GBps  |
| INSERT per/sec<br>Формула: RPS INSERT X WEIGHT      | 34,7   | 0,278  | 0,035 | 0,000 | 0,000 |
| READ per/sec<br>Формула: RPS READ X WEIGHT          | 3472,2 | 27,778 | 3,472 | 0,028 | 0,003 |
| Кол-во соединений<br>Формула: READ per/sec / WEIGHT | 3472,2 |        |       |       |

| Хранилище                                                                                       |              |            |           |         |        |      |       |        |          |           |             |
| ----------------------------------------------------------------------------------------------- | ------------ | ---------- | --------- | ------- | ------ | ---- | ----- | ------ | -------- | --------- | ----------- |
| Ед. измерения                                                                                   | KB           | Mb         | MB        | Gb      | GB     | Tb   | TB    | Pb     | PB       | Eb        | EB          |
| На горизонте 5 лет (Insert)<br>Формула:<br>INSERT per/sec X 5 лет X 365 дней X Кол-во сек в дне | 5475000000   | 43800000   | 5475000   | 43800   | 5475   | 43,8 | 5,475 | 0,0438 | 0,005475 | 0,0000438 | 0,000005475 |
| На горизонте 5 лет (Read)<br>Формула:<br>READ per/sec X 5 лет X 365 дней X Кол-во сек в дне     | 547500000000 | 4380000000 | 547500000 | 4380000 | 547500 | 4380 | 547,5 | 4,38   | 0,5475   | 0,00438   | 0,0005475   |

| Стоимость                    | $          |
| ---------------------------- | ---------- |
| Трафик (1GB = 0.1$)          | $54 750,00 |
| Хранение данных (1 TB = 30$) | $164,25    |

| Кол-во серверов                                                                                                |     |
| -------------------------------------------------------------------------------------------------------------- | --- |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер                                                         | 0,0 |
| Кол-во серверов для удержания соединений<br>MAX: 100 000 соединений<br>Формула:<br>Кол-во соединений / 100 000 | 0,0 |
| Кол-во серверов для записи в БД<br>MAX: 1GbE/SEC<br>Формула:<br>READ per/sec / 1 Gb                            | 0,0 |
| Всего серверов                                                                                                 | 0,1 |

***Можно заключить, что для таких показателей достаточно и одной рабочей станции за несколько тысяч долларов.***


***Первоочередные особенности***

1. Система сама по себе имеет довольно простой функционал — прочитать или создать для ссылки короткий алиас. Можно иметь два модуля: один отвечает на запросы пользователей, а второй генерит ссылки;
2. При минимальном масштабировании необходимо, чтобы алиасы не повторялись между серверами. Сервера должны синхронизироваться между собой;
3. Можно отразить, что нам стоит озаботиться отдельным кодирующим сервисом для их создания.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/short.png)

***Какие данные храним:***

для каждой короткой ссылки храним созданный ключ, даты создания/протухания, опционально id пользователя;
<br/>
можем хранить профиль пользователя с id и типичными анкетными данными (имя, почта, дата создания, время последнего посещения).
 

***Вывод:***

eсть всего две легковесные, просто связанные таблицы, чувствительных данных нет (денежные транзакции), а записей поступает миллиарды;
<br/>
ACID не критичен, JOIN-ы можно сделать программно, следовательно, колоночная БД типа Cassandra будет в самый раз.



**Хостинг текстов**

***Зачем нужен такой сервис?***
<br/>
Чтобы делиться кусками кода и искать в нём баги вместе.


***Функциональные требования***


1. Пользователи могут загрузить отрывок текста и получить URL для доступа к нему;
2. Пользователи могут загружать только текстовые данные;
3. У пользователей есть возможность выбрать синоним для доступа;
4. Ссылки «протухают» через какое-то время. Пользователи могут его изменять.


***Нефункциональные требования***

1. Высокая надёжность сервиса — загруженные данные не должны исчезать;
2. Высокая доступность сервиса — иначе доступ к нужным данным пропадает;
3. Сохранённые тексты должны отображаться с минимальной задержкой;
4. Короткие ссылки должны быть случайны настолько, чтобы их нельзя было подобрать.


| Вводные данные              | Значение   | Ед. измерения |
| --------------------------- | ---------- | ------------- |
| DAU<br>Daily active users   | 500 000,00 |               |
| INSERT<br>Записи на вставку | 2          |               |
| READ<br>Чтение записей      | 10         |               |
| WEIGHT<br>Вес одной записи  | 10         | KB            |


| Вычисления                                                    |        |
| ------------------------------------------------------------- | ------ |
| Вычислительная нагрузка                                       |        |
| RPS INSERT<br>Формула: (DAU X INSERT) / Кол-во сек в дне      | 11,57  |
| RPS READ<br>Формула: (DAU X INSERT X READ) / Кол-во сек в дне | 115,74 |


| Cеть / Трафик                                       |        |       |       |       |       |
| --------------------------------------------------- | ------ | ----- | ----- | ----- | ----- |
| Ед. измерения                                       | KB     | Mbps  | MBps  | Gbps  | GBps  |
| INSERT per/sec<br>Формула: RPS INSERT X WEIGHT      | 115,7  | 0,926 | 0,116 | 0,001 | 0,000 |
| READ per/sec<br>Формула: RPS READ X WEIGHT          | 1157,4 | 9,259 | 1,157 | 0,009 | 0,001 |
| Кол-во соединений<br>Формула: READ per/sec / WEIGHT | 115,7  |       |       |       |


| Хранилище                                                                                       |              |            |           |         |        |      |       |       |         |          |            |
| ----------------------------------------------------------------------------------------------- | ------------ | ---------- | --------- | ------- | ------ | ---- | ----- | ----- | ------- | -------- | ---------- |
| Ед. измерения                                                                                   | KB           | Mb         | MB        | Gb      | GB     | Tb   | TB    | Pb    | PB      | Eb       | EB         |
| На горизонте 5 лет (Insert)<br>Формула:<br>INSERT per/sec X 5 лет X 365 дней X Кол-во сек в дне | 18250000000  | 146000000  | 18250000  | 146000  | 18250  | 146  | 18,25 | 0,146 | 0,01825 | 0,000146 | 0,00001825 |
| На горизонте 5 лет (Read)<br>Формула:<br>READ per/sec X 5 лет X 365 дней X Кол-во сек в дне     | 182500000000 | 1460000000 | 182500000 | 1460000 | 182500 | 1460 | 182,5 | 1,46  | 0,1825  | 0,00146  | 0,0001825  |


| Стоимость                    | $          |
| ---------------------------- | ---------- |
| Трафик (1GB = 0.1$)          | $18 250,00 |
| Хранение данных (1 TB = 30$) | $547,50    |


| Кол-во серверов                                                                                                |     |
| -------------------------------------------------------------------------------------------------------------- | --- |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер                                                         | 0,1 |
| Кол-во серверов для удержания соединений<br>MAX: 100 000 соединений<br>Формула:<br>Кол-во соединений / 100 000 | 0,0 |
| Кол-во серверов для записи в БД<br>MAX: 1GbE/SEC<br>Формула:<br>READ per/sec / 1 Gb                            | 0,0 |
| Всего серверов                                                                                                 | 0,1 |



***Можно заключить, что для таких показателей достаточно и одной рабочей станции за несколько тысяч долларов.***


***Первоочередные особенности***

1. В отличие от простого сокращателя ссылок здесь можем хранить текст любого размера;
2. Стоит сразу подумать о том, что хранить большие записи и малые метаданные лучше отдельно;
3. Можно отразить, что будем использовать базу данных для метаданных и отдельное хранилище для текстов.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/text.png)

***Какие данные храним:***

для каждого текста храним созданный id и сам контент, даты создания/протухания, опционально id пользователя;
<br/>
можем хранить профили пользователя с id и типичными анкетными данными (имя, почта, даты создания и последнего посещения);
<br/>
если тексты достаточно большие, с опциональной привязкой картинок, можно подумать над их хранением отдельно.
  

***Вывод:***
есть всего две легковесные, просто связанные таблицы, чувствительных данных нет (денежные транзакции), а записей поступает миллиарды;
<br/>
ACID не критичен, JOIN-ы можно сделать программно, следовательно, колоночная БД типа Cassandra будет в самый раз. 
<br/>
Большие тексты и другие медиафайлы можно хранить в отдельном хранилище типа Amazon S3, повышая лимиты при необходимости.


**Автодополнение**

***Зачем нужен такой сервис?***
<br/>
Чтобы при вводе первых букв запроса предлагались варианты дополнения. Например, при поиске «Н» сразу дополнялось до «НФТ купить».

***Функциональные требования***

1. Пользователи вбивают запрос, для него предлагается топ-5 дополнений;
2. Варианты обновляются по мере вбивания букв и слов;
3. Пользователи могут видеть в подсказках предыдущие релевантные результаты;
5. Учитывать прошлую историю запросов, профиль пользователя, контекст.

***Нефункциональные требования***

1. Высокая отзывчивость сервиса — дополнения обновляются почти в реальном времени.


| Вводные данные                                                                 | Значение         | Ед. измерения |
| ------------------------------------------------------------------------------ | ---------------- | ------------- |
| UQ<br>Unique Query<br>Кол-во уникальных<br>обращений                           | 10 000 000,00    |               |
| DQC<br>Daily Query Count                                                       | 1 000 000 000,00 |               |
| READ<br>Чтение записей                                                         | 5                |               |
| WEIGHT<br>50 символов на запрос в среднем,<br>т.е. в 200 байт точно поместится | 0,2              | KB            |

| Вычисления                                                                                                                                                |           |         |        |      |       |       |        |          |           |             |              |
| --------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ------- | ------ | ---- | ----- | ----- | ------ | -------- | --------- | ----------- | ------------ |
| Хранилище                                                                                                                                                 |           |         |        |      |       |       |        |          |           |             |              |
| Ед. измерения                                                                                                                                             | KB        | Mb      | MB     | Gb   | GB    | Tb    | TB     | Pb       | PB        | Eb          | EB           |
| Хранение всех запросов за день<br>Формула:<br>UQ \* WEIGHT                                                                                                | 2000000   | 16000   | 2000   | 16   | 2     | 0,016 | 0,002  | 0,000016 | 0,000002  | 0,000000016 | 0,000000002  |
| Хранение на горизонте 5 лет<br>Каждый день появляется 5%<br>новых уникальных запросов<br>Формула:<br>Хранение за день + Хранение за день X 1825 дней Х 5% | 184500000 | 1476000 | 184500 | 1476 | 184,5 | 1,476 | 0,1845 | 0,001476 | 0,0001845 | 0,000001476 | 0,0000001845 |

| Стоимость                                              | $     |
| ------------------------------------------------------ | ----- |
| Хранение данных (1 TB = 30$)                           | $5,54 |
| Кол-во серверов                                        |       |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер | 0,0   |
| Всего серверов                                         | 0,0   |

***Можно заключить, что для таких показателей достаточно и одной рабочей станции за несколько тысяч долларов.***


***Первоочередные особенности***

1. Сервис под капотом хранит все существующие запросы из другой системы (поиска);
2. Мы должны на его основе заранее построить специальный индекс для поиска слов с префиксом;
3. Можно отразить, что при обращении к системе мы идём в поисковый индекс, построенный на основе запросов.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/auto.png)


**Облачный диск**

***Зачем нужен такой сервис?***
<br/>
Чтобы файлы были везде доступны.

***Функциональные требования***

1. Пользователи могут загружать и скачивать свои файлы с любого устройства;
2. Пользователи могут делиться файлами и директориями с другими;
3. Между всеми устройствами происходить автоматическая синхронизация;
4. Можно редактировать файл офлайн, синхронизация произойдет при выходе в сеть.

***Нефункциональные требования***

1. (A) Атомарность — файл либо обновляется, либо нет, но не становится «битым»;
2. (C) Консистентность — при любом сценарии работы данные в облаке всегда валидны;
3. (I) Изоляция — можно править разные файлы с разных устройств, всё будет хорошо;
4. (D) Устойчивость — после зелёной галочки на файле кофе уже можно выливать на ноутбук.


| Вводные данные                           | Значение      | Ед. измерения |
| ---------------------------------------- | ------------- | ------------- |
| Всего пользователей                      | 1 000 000 000 |               |
| Общее кол-во файлов<br>на 1 пользователя | 100           |               |
| DAU<br>Daily active users                | 100 000 000   |               |
| UPDATE<br>Записи на обновление           | 1             |               |
| WEIGHT<br>Вес одной записи               | 100           | KB            |

| Вычислительная нагрузка                                  |         |
| -------------------------------------------------------- | ------- |
| RPS UPDATE<br>Формула: (DAU X UPDATE) / Кол-во сек в дне | 1157,41 |


| Cеть / Трафик                                         |          |         |         |       |       |
| ----------------------------------------------------- | -------- | ------- | ------- | ----- | ----- |
| Ед. измерения                                         | KB       | Mbps    | MBps    | Gbps  | GBps  |
| UPDATE per/sec<br>Формула: RPS UPDATE X WEIGHT        | 115740,7 | 925,926 | 115,741 | 0,926 | 0,116 |
| Кол-во соединений<br>Формула: UPDATE per/sec / WEIGHT | 1157,4   |         |         |       |

| Хранилище                                                                                       |                |              |             |           |          |        |       |     |       |       |         |
| ----------------------------------------------------------------------------------------------- | -------------- | ------------ | ----------- | --------- | -------- | ------ | ----- | --- | ----- | ----- | ------- |
| Ед. измерения                                                                                   | KB             | Mb           | MB          | Gb        | GB       | Tb     | TB    | Pb  | PB    | Eb    | EB      |
| На горизонте 5 лет (UPDATE)<br>Формула:<br>UPDATE per/sec X 5 лет X 365 дней X Кол-во сек в дне | 18250000000000 | 146000000000 | 18250000000 | 146000000 | 18250000 | 146000 | 18250 | 146 | 18,25 | 0,146 | 0,01825 |
| Хранение всех файлов<br>Формула:<br>Всего пользователей X Всего файлов X Вес                    | 10000000000000 | 80000000000  | 10000000000 | 80000000  | 10000000 | 80000  | 10000 | 80  | 10    | 0,08  | 0,01    |


| Стоимость                    | $             |
| ---------------------------- | ------------- |
| Трафик (1GB = 0.1$)          | $1 825 000,00 |
| Хранение данных (1 TB = 30$) | $300 000,00   |

| Кол-во серверов                                                                                                |      |
| -------------------------------------------------------------------------------------------------------------- | ---- |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер                                                         | 50,0 |
| Кол-во серверов для удержания соединений<br>MAX: 100 000 соединений<br>Формула:<br>Кол-во соединений / 100 000 | 0,01 |
| Кол-во серверов для записи в БД<br>MAX: 1GbE/SEC<br>Формула:<br>READ per/sec / 1 Gb                            | 0,9  |
| Всего серверов                                                                                                 | 50,9 |

***Вывод***

На датаноды нам понадобятся до сотни серверов, соединения и трафик выдержит и один сервер, но лучше больше. Поддержка всей системы обойдётся в несколько миллионов долларов на горизонте пяти лет.

***Первоочередные особенности***

1. Файлы могут быть любого размера, стоит использовать отдельно БД для метаданных и файловое хранилище;
2. Клиентское приложение должно уметь показывать структуру файлов и при необходимости загружать/скачивать
3. При обновлении файлов другим клиентам приходят уведомления об этом;
4. Можно отразить использование 2-х типов хранилища и компоненты загрузки, обновления и синхронизации.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/disc.png)

***Какие данные храним:***

метаданные: информация про пользователей, устройства, которыми они пользуются, последнее время посещение, и файлы, разбитые на блоки вместе с хешами, для дедупликации;
сами блоки файлов как большие бинарные объекты в отдельном файловом хранилище.
 

***Вывод:***

метаданные про пользователей и используемые файлы будет легче хранить в РСУБД с ACID из-за чувствительности (оплата подписки за сервис);
<br/>
с другой стороны, из-за большого количества блоков и скорости их прирастания, будет сложнее масштабироваться;
<br/>
можно рассмотреть колоночные БД с программной «ACID-изацией» критических участков системы, связанных с синхронизацией и т.д.;
<br/>
файловые блоки можем хранить как объекты в отдельном хранилище типа AmazonS3, повышая лимиты при необходимости.


**Приложение для выкладывания фотографий**

***Зачем нужен такой сервис?***
<br/>
Чтобы делиться фотографиями в зеркале и с едой с друзьями.


***Функциональные требования***

1. Пользователи могут загружать, скачивать и просматривать фотографии;
2. Пользователи могут искать другие фотографии по описанию;
3. Пользователи могут подписываться на фотографии друг друга;
4. Есть лента из популярных фото всех отслеживаемых пользователей.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пользователи перейдут в Snapchat или TikTok;
2. Лента с фотографиями должна отображаться за сотни миллисекунд (иначе см п. 1);
3. Консистентность не так критична — подписчики сколько-то потерпят без ваших селфи;
4. Высокая надёжность сервиса — ни одно селфи или фото еды не должно потеряться.


| Вводные данные              | Значение       | Ед. измерения |
| --------------------------- | -------------- | ------------- |
| DAU<br>Daily active users   | 100 000 000,00 |               |
| INSERT<br>Записи на вставку | 1              |               |
| READ<br>Чтение записей      | 100            |               |
| WEIGHT<br>Вес одной записи  | 100            | KB            |


| Вычислительная нагрузка                                       |           |
| ------------------------------------------------------------- | --------- |
| RPS INSERT<br>Формула: (DAU X INSERT) / Кол-во сек в дне      | 1157,41   |
| RPS READ<br>Формула: (DAU X INSERT X READ) / Кол-во сек в дне | 115740,74 |



| Cеть / Трафик                                       |            |           |           |        |        |
| --------------------------------------------------- | ---------- | --------- | --------- | ------ | ------ |
| Ед. измерения                                       | KB         | Mbps      | MBps      | Gbps   | GBps   |
| INSERT per/sec<br>Формула: RPS INSERT X WEIGHT      | 115740,7   | 925,926   | 115,741   | 0,926  | 0,116  |
| READ per/sec<br>Формула: RPS READ X WEIGHT          | 11574074,1 | 92592,593 | 11574,074 | 92,593 | 11,574 |
| Кол-во соединений<br>Формула: READ per/sec / WEIGHT | 115740,7   |           |           |        |


| Хранилище                                                                                       |                |                |               |             |            |          |         |       |       |       |         |
| ----------------------------------------------------------------------------------------------- | -------------- | -------------- | ------------- | ----------- | ---------- | -------- | ------- | ----- | ----- | ----- | ------- |
| Ед. измерения                                                                                   | KB             | Mb             | MB            | Gb          | GB         | Tb       | TB      | Pb    | PB    | Eb    | EB      |
| На горизонте 5 лет (Insert)<br>Формула:<br>INSERT per/sec X 5 лет X 365 дней X Кол-во сек в дне | 18250000000000 | 146000000000   | 18250000000   | 146000000   | 18250000   | 146000   | 18250   | 146   | 18,25 | 0,146 | 0,01825 |
| На горизонте 5 лет (Read)<br>Формула:<br>READ per/sec X 5 лет X 365 дней X Кол-во сек в дне     | 1,825E+15      | 14600000000000 | 1825000000000 | 14600000000 | 1825000000 | 14600000 | 1825000 | 14600 | 1825  | 14,6  | 1,825   |


| Стоимость                    | $               |
| ---------------------------- | --------------- |
| Трафик (1GB = 0.1$)          | $182 500 000,00 |
| Хранение данных (1 TB = 30$) | $547 500,00     |

| Кол-во серверов                                                                                                |       |
| -------------------------------------------------------------------------------------------------------------- | ----- |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер                                                         | 91,3  |
| Кол-во серверов для удержания соединений<br>MAX: 100 000 соединений<br>Формула:<br>Кол-во соединений / 100 000 | 1,2   |
| Кол-во серверов для записи в БД<br>MAX: 1GbE/SEC<br>Формула:<br>READ per/sec / 1 Gb                            | 92,6  |
| Всего серверов                                                                                                 | 185,0 |


***Вывод***

На датаноды понадобятся до сотни серверов, на соединения с трафиком и чтение БД ещё десятки. Поддержка всей системы обойдётся в сотни миллионов долларов на горизонте пяти лет.


***Первоочередные особенности***

1. Основных сценариев использования приложения два — загрузка или просмотры фотографий;
2. Фотографии занимают несравнимо больше, чем метаданные о снимках, постах и самих пользователях;
3. Можно отразить оба сценария в виде микро-приложений и необходимость разделения хранилища.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/photo.png)

***Какие данные храним:***

стандартный профиль пользователя (имя, почта, аватарка, даты создания и последнего посещения);
<br/>
фотографии (медиафайлы) и метаданные для них (id автора, теги, геометка, дата публикации и т.п.);
<br/>
таблицы подписок на посты пользователей.
 

***Вывод:***

т.к. для создания ленты пользователей (и рекомендаций) нужны JOIN-ы, очевидным выбором будет РСУБД типа Postgres, MyQSLи т.д.;
<br/>
но популярные сервисы с фотографиями так или иначе придётся масштабировать, и с РСУБД это делать сложнее;
<br/>
можно рассмотреть сценарий с «программными» JOIN-ами, когда все метаданные постов хранятся в колоночной БД типа Cassandra;
<br/>
ленту пользователя с рекомендациями и постами друзей можно также предпосчитать программно, и кешировать в key-value хранилищах типа Redis.

**Телеграм**

***Зачем нужен такой сервис?***
<br/>
Чтобы от общения вас отвлекали с работы, где не купили Slack.

***Функциональные требования***

1. Можно общаться один на один в чатах с другими пользователями;
2. Можно понять, онлайн ли пользователь и когда был последний раз;
3. Сервис обеспечивает постоянное хранение всей истории переписки;
4. Cтикеры, групповые чаты, кружочки с видео, беседы как в Clubhouse, стена, истории.

***Нефункциональные требования***

1. Общение происходит в реальном времени с минимальными задержками;
2. Консистентность — одинаковые сообщения в чатах на всех устройствах;
3. Высокая доступность сервиса — иначе пользователи перейдут в там-там.

| Вводные данные              | Значение       | Ед. измерения |
| --------------------------- | -------------- | ------------- |
| DAU<br>Daily active users   | 100 000 000,00 |               |
| INSERT<br>Записи на вставку | 100            |               |
| READ<br>Чтение записей      | 10             |               |
| WEIGHT<br>Вес одной записи  | 1              | KB            |

| Вычислительная нагрузка                                       |            |
| ------------------------------------------------------------- | ---------- |
| RPS INSERT<br>Формула: (DAU X INSERT) / Кол-во сек в дне      | 115740,74  |
| RPS READ<br>Формула: (DAU X INSERT X READ) / Кол-во сек в дне | 1157407,41 |


| Cеть / Трафик                                       |           |          |          |       |       |
| --------------------------------------------------- | --------- | -------- | -------- | ----- | ----- |
| Ед. измерения                                       | KB        | Mbps     | MBps     | Gbps  | GBps  |
| INSERT per/sec<br>Формула: RPS INSERT X WEIGHT      | 115740,7  | 925,926  | 115,741  | 0,926 | 0,116 |
| READ per/sec<br>Формула: RPS READ X WEIGHT          | 1157407,4 | 9259,259 | 1157,407 | 9,259 | 1,157 |
| Кол-во соединений<br>Формула: READ per/sec / WEIGHT | 1157407,4 |          |          |       |

| Хранилище                                                                                       |                 |               |              |            |           |         |        |      |       |       |         |
| ----------------------------------------------------------------------------------------------- | --------------- | ------------- | ------------ | ---------- | --------- | ------- | ------ | ---- | ----- | ----- | ------- |
| Ед. измерения                                                                                   | KB              | Mb            | MB           | Gb         | GB        | Tb      | TB     | Pb   | PB    | Eb    | EB      |
| На горизонте 5 лет (Insert)<br>Формула:<br>INSERT per/sec X 5 лет X 365 дней X Кол-во сек в дне | 18250000000000  | 146000000000  | 18250000000  | 146000000  | 18250000  | 146000  | 18250  | 146  | 18,25 | 0,146 | 0,01825 |
| На горизонте 5 лет (Read)<br>Формула:<br>READ per/sec X 5 лет X 365 дней X Кол-во сек в дне     | 182500000000000 | 1460000000000 | 182500000000 | 1460000000 | 182500000 | 1460000 | 182500 | 1460 | 182,5 | 1,46  | 0,1825  |

| Стоимость                    | $              |
| ---------------------------- | -------------- |
| Трафик (1GB = 0.1$)          | $18 250 000,00 |
| Хранение данных (1 TB = 30$) | $547 500,00    |

| Кол-во серверов                                                                                                |       |
| -------------------------------------------------------------------------------------------------------------- | ----- |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер                                                         | 91,3  |
| Кол-во серверов для удержания соединений<br>MAX: 100 000 соединений<br>Формула:<br>Кол-во соединений / 100 000 | 11,6  |
| Кол-во серверов для записи в БД<br>MAX: 1GbE/SEC<br>Формула:<br>READ per/sec / 1 Gb                            | 9,3   |
| Всего серверов                                                                                                 | 112,1 |


***Вывод***

В итоге нам понадобятся сотни серверов и для датанод для удержания соединений и для записей в БД. Поддержка всей системы обойдётся в десятки миллионов долларов на горизонте пяти лет.

***Первоочередные особенности***

1. Пользователь может обращаться либо к истории сообщений, либо вести беседы;
2. При беседе двух пользователей необходимо быстро доставлять их сообщения друг другу;
3. Можно отразить необходимость взаимодействия серверов, к которым обращаются пользователи.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/tg.png)



***Какие данные храним:***

онлайн данные об установленных соединениях, открытых чатах, каналах и т.п.;
<br/>
данные о пользователе (профиль, аватарки, время посещения, настройки приложения и т.п.);
<br/>
история чатов, изображения и прочие прикреплённые медиа и просто файлы.
 

***Вывод:***

для онлайн данных нужно по ключам user id получать и изменять сведения о соединениях, для чего подойдёт key-value БД типа Redis;
<br/>
данные о пользователях и их связях (скорее всего «контактах») можно хранить в реляционных БД вроде Postgres, а аватарки в S3-подобных хранилищах;
<br/>
т.к. количество сообщений будет большим, всю историю сообщений можно хранить в колоночных БД, а недавние сообщения выкладывать заранее в Redis.


**Твиттер**

***Зачем нужен такой сервис?***
<br/>
Чтобы делиться короткими сообщениями со всеми.

***Функциональные требования***

1. Пользователи могут добавлять короткие посты;
2. Посты могут содержать фото или видео;
3. Пользователи могут отслеживать посты других;
4. Есть лента из твитов всех отслеживаемых пользователей;
5. Бонус: поиск, реплаи, горячие темы, уведомления, рекомендации, аватарки с NFT обезьянами.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пользователи вернутся в живой журнал;
2. Лента с твитами должна отображаться за сотни миллисекунд (иначе см п. 1);
3. Консистентность не так критична — подписчики сколько-то потерпят без ваших мыслей.


| Вводные данные              | Значение       | Ед. измерения |
| --------------------------- | -------------- | ------------- |
| DAU<br>Daily active users   | 100 000 000,00 |               |
| INSERT<br>Записи на вставку | 1              |               |
| READ<br>Чтение записей      | 100            |               |
| WEIGHT<br>Вес одной записи  | 10             | KB            |


| Вычисления                                               |           |
| -------------------------------------------------------- | --------- |
| Вычислительная нагрузка                                  |           |
| RPS INSERT<br>Формула: (DAU X INSERT) / Кол-во сек в дне | 1157,41   |
| RPS READ<br>Формула: (DAU X READ) / Кол-во сек в дне     | 115740,74 |

| Cеть / Трафик                                       |           |          |          |       |       |
| --------------------------------------------------- | --------- | -------- | -------- | ----- | ----- |
| Ед. измерения                                       | KB        | Mbps     | MBps     | Gbps  | GBps  |
| INSERT per/sec<br>Формула: RPS INSERT X WEIGHT      | 11574,1   | 92,593   | 11,574   | 0,093 | 0,012 |
| READ per/sec<br>Формула: RPS READ X WEIGHT          | 1157407,4 | 9259,259 | 1157,407 | 9,259 | 1,157 |
| Кол-во соединений<br>Формула: READ per/sec / WEIGHT | 115740,7  |          |          |       |

| Хранилище                                                                                       |                 |               |              |            |           |         |        |      |       |        |          |
| ----------------------------------------------------------------------------------------------- | --------------- | ------------- | ------------ | ---------- | --------- | ------- | ------ | ---- | ----- | ------ | -------- |
| Ед. измерения                                                                                   | KB              | Mb            | MB           | Gb         | GB        | Tb      | TB     | Pb   | PB    | Eb     | EB       |
| На горизонте 5 лет (Insert)<br>Формула:<br>INSERT per/sec X 5 лет X 365 дней X Кол-во сек в дне | 1825000000000   | 14600000000   | 1825000000   | 14600000   | 1825000   | 14600   | 1825   | 14,6 | 1,825 | 0,0146 | 0,001825 |
| На горизонте 5 лет (Read)<br>Формула:<br>READ per/sec X 5 лет X 365 дней X Кол-во сек в дне     | 182500000000000 | 1460000000000 | 182500000000 | 1460000000 | 182500000 | 1460000 | 182500 | 1460 | 182,5 | 1,46   | 0,1825   |

| Стоимость                    | $              |
| ---------------------------- | -------------- |
| Трафик (1GB = 0.1$)          | $18 250 000,00 |
| Хранение данных (1 TB = 30$) | $54 750,00     |

| Кол-во серверов                                                                                                |      |
| -------------------------------------------------------------------------------------------------------------- | ---- |
| Кол-во серверов для хранения<br>MAX: 200 TB - 1 сервер                                                         | 9,1  |
| Кол-во серверов для удержания соединений<br>MAX: 100 000 соединений<br>Формула:<br>Кол-во соединений / 100 000 | 1,2  |
| Кол-во серверов для записи в БД<br>MAX: 1GbE/SEC<br>Формула:<br>READ per/sec / 1 Gb                            | 9,3  |
| Всего серверов                                                                                                 | 19,5 |

***Вывод***

В итоге нам понадобятся десяток серверов для датанод + сервера вычислений и вспомогательные сервера. Поддержка всей системы обойдётся в десятки миллионов долларов на горизонте пяти лет.

***Первоочередные особенности***

1. Огромное количество пользователей, а значит, необходимость масштабирования;
2. Посты могут содержать тяжелые фото или видео, которые лучше хранить отдельно от данных про пользователей;
3. Можно сразу отметить наличие балансировщика нагрузки, а также наличие разных типов хранилищ.


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/twit.png)


***Какие данные храним:***

стандартный профиль пользователя (имя, почта, аватарка, дата создания и последнего посещения);
<br/>
текст твита, медиафайлы + метаданные (id автора, теги, геометка, даты, лайки и т.п.);
<br/>
таблицы подписок и избранных постов.
 

***Вывод:***

в целом, требования очень похожи на фотоаппы – профили, подписки и лайки можно хранить в Postgres, а твиты в HBase + Redis
<br/>
ленту пользователя можно также предпосчитать и кешировать опять же в key-value хранилище типа Redis

**Нетфликс**

***Зачем нужен такой сервис?***
<br/>
Чтобы смотреть любимые фильмы, сериалы и передачи.

***Функциональные требования***

1. Пользователи могут просматривать фильмы и сериалы;
2. Пользователи могут искать контент по названию;
3. Сервис отслеживает оценки и статистику просмотров;
4. Пользователям рекомендуются новые фильмы и сериалы;
5. Бонус: жанры, популярное, избранное, списки на посмотреть потом.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пользователи будут смотреть котиков у конкурентов;
2. Высокая отзывчивость сервиса — видео должны проигрываться без подвисаний;
3. Консистентность не так критична — после недельного ожидания одна минута не заметна.


***DAU*** = 10 миллионов  
<br/>
***фильмов в библиотеке*** = 20000 
<br/>
***Каждый пользователь смотрит пару серий или фильм в течение 1 часа***
<br/>
***Видео*** = 1080p с битрейтом 10 Mbps

***Сеть*** 
<br/>
***трафик*** = 10M x 3600 (сек/час) x 10 Mbps = 40 PB (40 000 000 GB) в день!

***Вычисления***
10M x 10 (серий) / 84600 (сек в день) = 1k RPS на получение метаданных между сериями.


***Хранилище***
<br/>
20000 наименований по 10 часов (фильмы, но и сериалы) 20k x 10 x 3600 (сек/час)  x (50 + 10 + 5 + …) Mbps (версии разрешений)~= 5 PB.

***Стоимость***
<br/>
На хранилище нужно потратить десятки тысяч долларов, а на трафик миллиарды.

За трафик по стандартному прайсу расплатиться просто невозможно, поэтому этого и не происходит , клиент качает трафик с сервера своего провайдера:)



***Первоочередные особенности***

1. Пользователи могут просматривать как информация о фильмах, так и несравнимо более тяжёлые фильмы как таковые;
2. Пользователи могут смотреть фильмы с разных устройств, поэтому нужно подготовить версии фильмов разного размера;
3. Можно отметить использование нескольких типов хранилищ и необходимость подготовки фильмов под разные форматы.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/net.png)


***Какие данные храним:***

стандартные профили пользователей, справочную информацию о фильмах, оценки и добавление в избранное;
<br/>
разнообразная статистика по просмотрам для аналитики и построения рекомендаций;
<br/>
сами фильмы в разных разрешениях, звуковые дорожки и субтитры к ним.

***Вывод:***

для хранения стандартных табличных данных можно использовать РСУБД типа Postgres, частые фильмы положить в кеш (Redis);
<br/>
для аналитики и рекомендаций нужно будет собирать много логов в колоночную БД типа Cassandra;
<br/>
готовые рекомендации — в Redis. Сами видеофайлы и сопроводительные материалы нужно будет хранить в хранилищах типа S3, желательно геораспределённых.

**Поиск ресторанов**

***Зачем нужен такой сервис?***
<br/>
Чтобы поесть не холодную еду из зелёных и жёлтых сумок.

***Функциональные требования***

1. Пользователи могут добавлять/обновлять/удалять заведения;
2. Пользователи могут находить подходящие заведения поблизости;
3. Пользователи могут оставлять отзывы к посещённым местам с фото и оценками.
4. Бонус: фильтры по категориям, рекомендации новых заведений.

***Нефункциональные требования***

1. Поиск должен происходить быстро, иначе пользователь закажет доставку;
2. Гораздо большая нагрузка ожидается на подсервис поиска, чем на редактирование.



**Сервис такси**

***Зачем нужен такой сервис?***
<br/>
Чтобы легко доехать из точки А в точку Б с водителем.

***Функциональные требования***

1. Водители могут выходить на смену и ждать заказ;
2. Пассажиры могут находить водителей поблизости;
3. Пассажиры могут заказывать поездку, для неё находится водитель;
4. Позиции отслеживаются с момента принятия поездки и до её окончания. В конце поездки водитель продолжает смену;
5. Бонус: оценки водителям, оценки пассажирам, объяснение ценообразования.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пассажиры поедут на метро, а водители на вокзал;
2. Низкое время ожидания для пассажира, маленький простой для водителя.


***Первоочередные особенности***

1. Есть фактически отдельные сервисы для водителя и пассажира как пользователей сервиса;
2. Нужно как обрабатывать текущую поездку, так и хранить исторические вместе с информацией о пользователях;
3. Можно отразить раздельность сервисов, необходимость ведения поездок и хранения истории.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/taxi.png)

***Какие данные храним:***

профили пользователей, настройки приложений;
<br/>
онлайн данные о позициях пользователей и текущих поездках;
<br/>
офлайн данные об истории поездок, чат с поддержкой.

***Вывод:***

стандартные табличные данные с профилями и настройками — подойдёт РСУБД типа Postgres;
<br/>
данные о подключённых пользователях и текущих поездках храним в key-value БД, позиции водителей в специализированной гео-БД;
<br/>
при окончании поездки нуждающиеся в ACID обновления делаем в Postgres и кладём поездку в Cassandra, а недавние храним в Redis.