**Расчет нагрузки на систему**

Важно оценить необходимую инфраструктуру для разворачивания и поддержки системы, а также стоимость всего оборудования и процессов.

Хватит ли у заказчика материальных ресурсов, чтобы воплотить все сформулированные пожелания и требования к системе.

***Оценка позволит скорректировать ожидания от продукта.***


**Пользовательский трафик**
<br/>
Минимальный набор метрик, которые необходимо определить

1. ***MAU*** - количество активных пользователей в месяц 
2. ***DAU*** - ежедневное количество активных пользователей 
3. общее количество пользователей на каком-то горизонте (например, 5 лет)
4. Сколько какого контента в среднем производит типичный пользователь в день

***Отталкиваясь от входных бизнес-показателей мы сможем перейти к оценкам
нагрузки:***

1. ***RPS*** - какое максимальное число запросов в секунду будет к частям системы
2. Какое количество одновременных соединений нужно будет удерживать
3. Какой будет нагрузка на сетевой канал и какой общий трафик мы потратим
4. Какое будет приращение занимаемого места и сколько данных накопится
5. Во-сколько нам обойдутся трафик и железо, которые смогут это все поддерживать


**Сетевая нагрузка**
<br/>
Cостоит из удерживаемых соединений в единицу времени и общего объёма трафика, который может пройти через сеть за единицу времени.

***Ограничения по количеству удерживаемых соединений***

Современный сервер может выдержать наплыв в 10 тысяч — 100 тысяч соединений. Если по вашим расчётам количество пользователей выйдет за этот диапазон, следует заложиться на расширение серверных мощностей.



***Ограничения по трафику***

Максимальный предел трафика упирается в пропускную способность выбранной технологии и канала связи.

Вот некоторые данные по пропускной способности:

***Облако*** — до 1GbE в секунду;

***Витая пара*** — до 10GbE в секунду;

***Оптоволокно стандарта QSFP+*** - до 40 GbE;

***Infiniband (IB) — 100+ GB.***  Этот вид соединений используется в суперкомпьютерах и в огромных дата-центрах (очень дорогой вариант).

Также можно вывести общую стоимость трафика. Облачные провайдеры, например, оценивают 1GB трафика от 1 до 10 центов. Таким образом, зная общий трафик за месяц можно оценить стоимость месячного трафика необходимого для функционирования сервиса.


***Вычислительная нагрузка***
<br/>
Под вычислительной нагрузкой обычно подразумевается вычисление количества одновременных запросов пользователей к сервису. Этот показатель исчисляется в RPS (requests per second).

По грубой оценке можно исходить из того, что в простых сценариях ***в облаке** мы сможем выдерживать нагрузку в 
<br/>
***100k RPS*** для текстовых данных
<br/>
***10k RPS*** при чтении в БД
<br/>
***1k RPS*** при записи в БД.

При наличии мощных ***физических серверов*** в тех же сценариях можно прикидывать вероятную нагрузку в 
<br/>
***500k RPS*** для текстовых данных
<br/>
***50k RPS*** при чтении в БД
<br/>
***5k RPS*** при записи в БД.


***Нагрузка на хранилище***
<br/>
Оценив количество создаваемого контента со стороны пользователей, мы можем прикинуть, сколько места он будет занимать, какой постоянный прирост при этом ожидается и, соответственно, сколько того или иного железа нам потребуется для нашей системы.


***HDD*** со скоростью 100-300 МБ/с, каждый диск хранит до 20 ТБ по цене до $500;

***NVMe SSD*** со скоростью 3-5 ГБ/с, каждый диск хранит до 8 TБ по цене до $2000;

***Оперативная память (RAM)*** со скоростью 50 ГБ/с, стоимость планки 128 ГБ около $1000.


В среднем хранение 1 ТБ обойдётся в около $10000 в RAM, $300 на SSD и $30 на HDD.

Можно прикидывать, что в одном сервере будет до 1 ТБ RAM, 50 ТБ SSD или 200 ТБ HDD.

Если дисков много, в игру вступает и среднее количество отказов за год (AFR) в размере 1%.




**Выводы**
<br/>
На какие показатели стоит ориентироваться при расчётах нагрузки на систему:

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/mbgbtb.jpeg)

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/mbps.jpeg)


1. Трафик: MAU, DAU, прирост контента в день, всего за 5 лет, соотношение R/W;
2. Сеть: С10k давно не вопрос, 1 Gb p/s даже в облаке, платим не более $0.1/GB;
3. Вычисления: отдаём 100к текста, считываем 10к и записываем 1к простых запросов в БД;
4. Хранилище: HDD 300 MB/s и $30/TB, SSD 5 GB/s и $300/TB, RAM 50GB/s и $10000/TB;
5. В одну машину при этом можно поставить до 1 TB RAM, 50 TB SSD или 200 TB HDD.



**Высокоуровневый дизайн**

Под высокоуровневым дизайном мы будем понимать такой дизайн, который минимально учитывает основные сценарии взаимодействия с системой, но при этом не отражает всю сложность взаимодействия между большим количеством реальных компонентов и не принимает во внимание ту нагрузку, под которой система будет в продуктивной среде.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/design.png)

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/design2.png)


***Общий дизайн***
<br/>
В голове можно всегда держать общий дизайн, подходящий почти для всего:

1. Часть системы взаимодействует непосредственно с юзером (какой-то front-end);
2. Часть системы работает скрыто от пользователя под капотом (какой-то back-end);
3. Часть системы заточена на долгосрочное хранение данных (базы данных, файловые системы и т.п.).


**Требования ACID — это набор требований, которые обеспечивают сохранность ваших данных.**

1. ***Atomicity — Атомарность.*** Гарантирует, что каждая транзакция будет выполнена полностью или не будет выполнена совсем. Не допускаются промежуточные состояния.
2. ***Consistency — Согласованность.*** Это свойство вытекает из предыдущего. Благодаря тому, что транзакция не допускает промежуточных результатов, база остается консистентной. Есть такое определение транзакции: «Упорядоченное множество операций, переводящих базу данных из одного согласованного состояния в другое». То есть до выполнения операции и после база остается консистентной.
3. ***Isolation — Изолированность.*** Во время выполнения транзакции параллельные транзакции не должны оказывать влияния на её результат.
4. ***Durability — Надёжность.*** Если пользователь получил подтверждение от системы, что транзакция выполнена, он может быть уверен, что сделанные им изменения не будут отменены из-за какого-либо сбоя. Обесточилась система, произошел сбой в оборудовании? На выполненную транзакцию это не повлияет.

**Основные виды БД**

1. Реляционные — базы данных, записи в которых хранятся в виде набора таблиц и заранее определённых связей между ними. К таким БД относятся все СУБД, а также базы удовлетворяющее ACID-требованиям.
2. БД типа key-value — поддерживают и хранят только наиболее простые операции с записями по ключу.
3. Колоночные БД — данные хранятся в виде набора колонок, не связанных между собой. Каждая запись, которую нужно сохранить в базу, разбивается на части, каждая часть записывается в свою колонку.
4. Документные БД — хранят данные в виде сложных древовидных структур типа JSON.
5. Графовые базы данных - вершины, ребра и их свойства.
![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/graf.png)



**Реляционные базы данных**

Реляционные БД — базы данных, основанные на реляционной модели, подразумевающей организацию данных в виде таблиц (строк-записей и столбцов-атрибутов), каждая запись в которой имеет уникальный ключ и может в атрибутах ссылаться на ключи в других таблицах. Можно считать это «базой данных по умолчанию», т.к. при создании MVP вашего проекта вы наверняка подключите БД вроде SQLite, Postgres, MySQL и т.п. для хранения ваших записей.
<br/>
Все реляционные базы данных соблюдают ACID-требования.



**БД вида key-value**

База данных на основе пар «ключ‑значение» — это тип нереляционных баз данных, в котором для хранения данных используется простой метод «ключ‑значение». База данных на основе пар «ключ‑значение» хранит данные как совокупность пар «ключ‑значение», в которых ключ служит уникальным идентификатором. Как ключи, так и значения могут представлять собой что угодно: от простых до сложных составных объектов.

Базы данных с использованием пар «ключ‑значение» поддерживают высокую разделяемость и обеспечивают беспрецедентное горизонтальное масштабирование, недостижимое при использовании других типов баз данных. Например, Amazon DynamoDB выделяет дополнительные разделы на таблицу, если существующий раздел заполняется до предела и требуется больше пространства для хранения.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/key.png)

***Примеры использования*** 

***Хранилище сессий Redis***

Основанное на сессиях приложение (например, интернет‑приложение) запускает сессию, когда пользователь входит в систему. Оно активно до тех пор, пока пользователь не выйдет из системы или не истечет время сессии. 
<br/>
В течение этого периода приложение хранит все связанные с сессией данные либо в основной памяти, либо в базе данных. 
<br/>
Данные сессии могут включать информацию профиля пользователя, сообщения, индивидуальные данные и темы, рекомендации, таргетированные рекламные кампании и скидки.
<br/>
Каждая сессия пользователя имеет уникальный идентификатор. Данные сессий всегда запрашиваются только по первичному ключу, поэтому для их хранения отлично подходит быстрое хранилище пар «ключ‑значение». 
<br/>
В целом базы данных на основе пар «ключ‑значение» могут снижать накладные расходы в расчёте на страницу по сравнению с реляционными базами данных.


***Корзина интернет‑магазина MemcacheDB***

Во время праздничного сезона покупок сайт интернет‑магазина может получать миллиарды заказов за считанные секунды. 
<br/>
Используя базы данных на основе пар «ключ‑значение», можно обеспечить необходимое масштабирование при существенном увеличении объемов данных и чрезвычайно интенсивных изменениях состояния. 
<br/>
Такие базы данных позволяют одновременно обслуживать миллионы пользователей благодаря распределённым обработке и хранению данных. 
<br/>
Базы данных на основе пар «ключ‑значение» также обладают встроенной избыточностью, что позволяет справляться с потерей узлов хранилища.


***Колоночные базы данных***

Колоночные БД призваны решить проблему неэффективной работы традиционных СУБД в аналитических системах и системах c подавляющим большинством операций типа «чтение». 
<br/>
Они позволяют на более дешёвом и маломощном оборудовании получить прирост скорости выполнения запросов в 5, 10 и иногда даже в 100 раз, при этом, благодаря компрессии, данные будут занимать на диске в 5-10 раз меньше, чем в случае с традиционными БД.

У колоночных БД есть и недостатки — они медленно работают на запись, не подходят для транзакционных систем и, как правило, ввиду «молодости» имеют ряд ограничений для разработчика, привыкшего к развитым традиционным БД.

Обычно колоночные БД применяются в аналитических системах класса business intelligence (ROLAP) и аналитических хранилищах данных (data warehouses). Причём объёмы данных могут быть достаточно большими — есть примеры по 300-500ТБ и даже случаи с >1ПБ данных.

Примеры БД:
<br/>
Hbase
<br/>
Vertica (работает с VPN)
<br/>
ClickHouse

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/column.png)


***Документные базы данных***

Документная база данных — это тип нереляционных баз данных, предназначенный для хранения и запроса данных в виде документов в формате, подобном JSON.

Документные базы данных позволяют разработчикам хранить и запрашивать данные в БД с помощью той же документной модели, которую они используют в коде приложения. Гибкий, полуструктурированный, иерархический характер документов и документных баз данных позволяет им развиваться в соответствии с потребностями приложений.

Документная модель хорошо работает в таких примерах использования, как каталоги, пользовательские профили и системы управления контентом, где каждый документ уникален и изменяется со временем. Документные базы данных обеспечивают гибкость индексации, производительность выполнения стандартных запросов и аналитику наборов документов.

В следующем примере документ в формате, подобном JSON, описывает книгу.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/doc.png)


***Примеры использования***

***Управление контентом***

Документная база данных – отличный выбор для приложений управления контентом, таких как платформы для блогов и размещения видео. При использовании документной базы данных каждая сущность, отслеживаемая приложением, может храниться как отдельный документ.

Такой вид базы данных позволяет разработчику с удобством обновлять приложение при изменении требований. Кроме того, если необходимо изменить модель данных, то требуется обновление только затронутых этим изменением документов. Для внесения изменений нет необходимости обновлять схему и прерывать работу базы данных.


***Каталоги***

Документные базы данных эффективны для хранения каталожной информации. 
<br/>
Например, в приложениях для интернет‑коммерции разные товары обычно имеют различное количество атрибутов.
<br/>
Управление тысячами атрибутов в реляционных базах данных неэффективно. Кроме того, количество атрибутов влияет на производительность чтения. 
<br/>
При использовании документной базы данных атрибуты каждого товара можно описать в одном документе, что упрощает управление и повышает скорость чтения. 
<br/>
Изменение атрибутов одного товара не повлияет на другие товары.

Из примеров:
<br/>
MongoDB
<br/>
CouchDB
<br/>
Amazon DocumentDB



***CAP теорема***

В CAP говорится, что в распределённой системе возможно выбрать только два из трёх свойств:

1. C (consistency) — согласованность. Каждое чтение даст вам самую последнюю запись.
2. A (availability) — доступность. Каждый узел (не упавший) всегда успешно выполняет запросы (на чтение и запись).
3. P (partition tolerance) — устойчивость к распределению. Даже если между узлами нет связи, они продолжают работать независимо друг от друга.

***PACELC-теорема***

Вся теорема сводится к if P -> (A or C), else (L or C), или же:

В случае наличия разделения (P) надо выбирать между доступностью (A) и согласованностью (C).

Иначе (Else) мы можем дополнительно выбирать между меньшей задержкой (L) и согласованностью (C).

Latency — это время, за которое клиент получит ответ и которое регулируется каким-либо уровнем consistency. Latency в некотором смысле представляет собой степень доступности.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/PACELC.png)


***Выбор подходящей БД***

При проектировании подсистемы требуется определиться, что из C, A и P нам важнее:

***У согласованных и доступных систем (CA)*** есть сложности с устойчивостью к разделению, которые частично решается созданием реплик, а также такие системы масштабируют вертикально (мощнее железо), чтобы изначально избежать разделения. Именно сюда попадают RDBMS с ACID, которые всегда кстати для хранения чувствительных данных.

***В согласованных и устойчивых к разделению системах (CP)*** могут быть проблемы с доступностью, на зато данные в разделённых нодах всегда корректны и выглядят одинаково. Примеры таких БД есть в разных семействах — колоночная HBase, документная Mongo и Redis.

***В доступных и устойчивых к разделению системах (AP)*** нет строгой согласованности, но она так или иначе достигается, чего бывает достаточно для крупных систем (например, чей-то новый пост на другом конце света увидят на минуту позже). 
<br/>
Примеры опять же есть разные — колоночная Cassandra, документная CouchDB и Dynamo. В противовес к строгим ACID выделяют BASE (basic availability, soft state, eventual consistency).

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/bdchose.png)

Документные БД - для хранения атрибутов товаров 

Реляционные БД - для хранение баланса монет у игроков

Колоночные БД - для хранение истории кликов на сайте

Key-value БД - для хранения статусов пользователей




**Сокращатель ссылок**

***Зачем нужен данный сервис?***
<br/>
Чтобы превращать длинные ссылки в короткие и делиться ими.

***Функциональные требования***


1. По полной ссылке пользователь может сгенерировать короткую;
2. Когда пользователь переходит по короткой ссылке, он перенаправляется на исходную;
3. У пользователей есть возможность самим выбрать сокращённый синоним;
4. Ссылки «протухают» через какое-то время. Пользователи могут его изменять;
5. Cбор аналитики по переходам.



***Нефункциональные требования***


1. Высокая доступность сервиса — иначе ссылки просто перестают работать;
2. Перенаправление должно происходит с минимальной задержкой;
3. Короткие ссылки должны быть случайны настолько, чтобы их нельзя было подобрать.
4. Предоставление API для разработчиков.

***Вводные данные***

***Сокращений каждый месяц (ShortMonth)*** = 100 000 000 (100М)
<br/>
***DAU*** = 1 000 000
<br/>
***MAU*** = 10 000 000
<br/>
***Сокращений в среднем / 1 пользователь (AVG)*** = 3
<br/>
***Соотношение чтения и записи (Read/Insert)*** = 100 : 1

***Вычисления***

***Кол-во сокращений в месяц*** = AVG * DAU * 31 день ≈ 100 000 000

***Обращений к сервису в месяц*** = Read * ShortMonth = 100 x 100M = 10 миллиардов.

***Нагрузка на создание записей*** = ShortMonth / (месяц / кол-во сек в дне) = 100M / (30 x 86400) ≈ 40 RPS

***Нагрузка чтение записей*** = 40 RPS * 100 = 4000 RPS

***Вес 1 записи*** =  1 KB 
<br/>
***Генерирация трафика в секундах*** = 4040 RPS * 1 KB ≈ 4 Mb/s (Мегабайт в секунду) = 4 * 8 = 32 Mbps (Мегабит в секунду).

***На горизонте 5 лет*** 
<br/>
***Будем хранить*** = ShortMonth * 5 лет * 12 месяцев = 100M x 5 x 12 = 6 миллиардов записей.
<br/>
6 x 10^9 x 1 KB = 6 TB места.

***Можно заключить, что для таких показателей достаточно и одной рабочей станции за несколько тысяч долларов.***


***Первоочередные особенности***

1. Система сама по себе имеет довольно простой функционал — прочитать или создать для ссылки короткий алиас. Можно иметь два модуля: один отвечает на запросы пользователей, а второй генерит ссылки;
2. При минимальном масштабировании необходимо, чтобы алиасы не повторялись между серверами. Сервера должны синхронизироваться между собой;
3. Можно отразить, что нам стоит озаботиться отдельным кодирующим сервисом для их создания.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/short.png)

***Какие данные храним:***

для каждой короткой ссылки храним созданный ключ, даты создания/протухания, опционально id пользователя;
<br/>
можем хранить профиль пользователя с id и типичными анкетными данными (имя, почта, дата создания, время последнего посещения).
 

***Вывод:***

eсть всего две легковесные, просто связанные таблицы, чувствительных данных нет (денежные транзакции), а записей поступает миллиарды;
<br/>
ACID не критичен, JOIN-ы можно сделать программно, следовательно, колоночная БД типа Cassandra будет в самый раз.



**Хостинг текстов**

***Зачем нужен такой сервис?***
<br/>
Чтобы делиться кусками кода и искать в нём баги вместе.


***Функциональные требования***


1. Пользователи могут загрузить отрывок текста и получить URL для доступа к нему;
2. Пользователи могут загружать только текстовые данные;
3. У пользователей есть возможность выбрать синоним для доступа;
4. Ссылки «протухают» через какое-то время. Пользователи могут его изменять.


***Нефункциональные требования***

1. Высокая надёжность сервиса — загруженные данные не должны исчезать;
2. Высокая доступность сервиса — иначе доступ к нужным данным пропадает;
3. Сохранённые тексты должны отображаться с минимальной задержкой;
4. Короткие ссылки должны быть случайны настолько, чтобы их нельзя было подобрать.



***Вводные данные***

***DAU*** = 500 000
<br/>
***MAU*** = 10 000 000
<br/>
***Загрузки в среднем / 1 пользователь (AVG)*** = 2
<br/>
***Соотношение чтения и записи (Read/Insert)*** = 10 : 1

***Вычисления***

***Загрузок каждый день (Loadings)*** = DAU * AVG = 500 000 * 2 = 1 000 000

***Обращений к сервису в месяц*** = 30 дней * Loadings * Read =  30 * 1 000 000 * 10 = 300 миллионов

***Нагрузка на создание записей в день*** = Loadings / кол-во сек в дне = 1 000 000 / 86400 ≈ 12 RPS

***Нагрузка на создание чтение в день*** = 12 RPS * 10 (Read) = 120 RPS

***Вес 1 записи*** =  10 KB 

***Генерирация трафика в секундах*** = 132 RPS * 10 KB ≈ 1 Mb/s (Мегабайт в секунду) = 1 * 8 = 8 Mbps (Мегабит в секунду).

***На горизонте 5 лет*** 
<br/>
***Будем хранить*** = Loading * 30  дней * 5 лет * 12 месяцев = 1M x 30 x 5 x 12 ≈ 2 миллиарда записей.
<br/>
2 x 10^9 x 10 KB = 20 TB места.

***Можно заключить, что для таких показателей достаточно и одной рабочей станции за несколько тысяч долларов.***


***Первоочередные особенности***

1. В отличие от простого сокращателя ссылок здесь можем хранить текст любого размера;
2. Стоит сразу подумать о том, что хранить большие записи и малые метаданные лучше отдельно;
3. Можно отразить, что будем использовать базу данных для метаданных и отдельное хранилище для текстов.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/text.png)

***Какие данные храним:***

для каждого текста храним созданный id и сам контент, даты создания/протухания, опционально id пользователя;
<br/>
можем хранить профили пользователя с id и типичными анкетными данными (имя, почта, даты создания и последнего посещения);
<br/>
если тексты достаточно большие, с опциональной привязкой картинок, можно подумать над их хранением отдельно.
  

***Вывод:***
есть всего две легковесные, просто связанные таблицы, чувствительных данных нет (денежные транзакции), а записей поступает миллиарды;
<br/>
ACID не критичен, JOIN-ы можно сделать программно, следовательно, колоночная БД типа Cassandra будет в самый раз. 
<br/>
Большие тексты и другие медиафайлы можно хранить в отдельном хранилище типа Amazon S3, повышая лимиты при необходимости.


**Автодополнение**

***Зачем нужен такой сервис?***
<br/>
Чтобы при вводе первых букв запроса предлагались варианты дополнения. Например, при поиске «Н» сразу дополнялось до «НФТ купить».

***Функциональные требования***

1. Пользователи вбивают запрос, для него предлагается топ-5 дополнений;
2. Варианты обновляются по мере вбивания букв и слов;
3. Пользователи могут видеть в подсказках предыдущие релевантные результаты;
5. Учитывать прошлую историю запросов, профиль пользователя, контекст.

***Нефункциональные требования***

1. Высокая отзывчивость сервиса — дополнения обновляются почти в реальном времени.


***Вводные данные***

***Обращений к сервису в день*** = 1 миллиард 
<br/>
10 миллионов уникальных запросов покрывают большую часть обращений
<br/>
Запросы состоят в среднем не более чем из 5 слов длины 10.
<br/>
***Имеем: 50 символов на запрос в среднем, т.е. в 200 байт точно поместится.***

***Вычисления***

***Хранение всех запросов за день*** = 200 байт * 10 000 000 = 2 GB

***На горизонте 5 лет*** 
<br/>
Если предположить, что каждый день появляется 5% новых уникальных запросов

***индекс (GB в день)*** = 2GB + 1800 дней * 5% * 2GB = 182 GB 

***Можно заключить, что для таких показателей достаточно и одной рабочей станции за несколько тысяч долларов.***


***Первоочередные особенности***

1. Сервис под капотом хранит все существующие запросы из другой системы (поиска);
2. Мы должны на его основе заранее построить специальный индекс для поиска слов с префиксом;
3. Можно отразить, что при обращении к системе мы идём в поисковый индекс, построенный на основе запросов.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/auto.png)


**Облачный диск**

***Зачем нужен такой сервис?***
<br/>
Чтобы файлы были везде доступны.

***Функциональные требования***

1. Пользователи могут загружать и скачивать свои файлы с любого устройства;
2. Пользователи могут делиться файлами и директориями с другими;
3. Между всеми устройствами происходить автоматическая синхронизация;
4. Можно редактировать файл офлайн, синхронизация произойдет при выходе в сеть.

***Нефункциональные требования***

1. (A) Атомарность — файл либо обновляется, либо нет, но не становится «битым»;
2. (C) Консистентность — при любом сценарии работы данные в облаке всегда валидны;
3. (I) Изоляция — можно править разные файлы с разных устройств, всё будет хорошо;
4. (D) Устойчивость — после зелёной галочки на файле кофе уже можно выливать на ноутбук.


***Вводные данные***

***Пользователей всего*** = 1 миллиард 
<br/>
***DAU*** = 100M
<br/>
***Устройств пользователя*** = 5
<br/>
***Кол-во файлов / пользователь*** = 100 файлов по 100 килобайт, активные меняют 1 файл.

***Вычисления***

***Сеть*** 
<br/>
***трафик в день*** = DAU *  = 100M * 100 КБ = 10TB 
<br/>
***трафик в  секунду*** 10TB (10 000 GB = 80 000 Gb) / 86400 (сек / день) ≈ 1 Gbps
<br/>
***Кол-во соединений*** = 1 Gbps (125000 KB) / 100 KB = 1250 файлов = 1250 Соединений
<br/>
***На горизонте 5 лет*** = 5 x 365 x 10TB = 18 PB
<br/>
***RPS*** = 100M / 86400 (сек / день) ≈ 1000 с записями в БД
<br/>
***Хранилище*** = 1 миллиард пользователей * 100 килобайт = 10 PB

***Стоимость*** 

***Цена трафика*** = 0.1$ * 18 PB = 1 800 000 $
<br/>
(1GB трафика от 1 до 10 центов)

***Цена хранилища*** = 10 PB (10 000 TB)* 30$ = 300 000$
<br/>
В среднем хранение 1 ТБ обойдётся в около $30 на HDD.

***Вывод***

Хранение данных = 10 PB (10 000 TB), в одном сервере будет до 1 ТБ RAM, 50 ТБ SSD или 200 ТБ HDD. Необходимо < 100 серверов

Современный сервер может выдержать наплыв в 10 тысяч — 100 тысяч соединений и
<br/>
до 1GbE трафика в секунду
<br/>
соединения и трафик выдержит и один сервер, но лучше больше. 


Поддержка всей системы обойдётся в несколько миллионов долларов на горизонте пяти лет.


***Первоочередные особенности***

1. Файлы могут быть любого размера, стоит использовать отдельно БД для метаданных и файловое хранилище;
2. Клиентское приложение должно уметь показывать структуру файлов и при необходимости загружать/скачивать
3. При обновлении файлов другим клиентам приходят уведомления об этом;
4. Можно отразить использование 2-х типов хранилища и компоненты загрузки, обновления и синхронизации.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/disc.png)

***Какие данные храним:***

метаданные: информация про пользователей, устройства, которыми они пользуются, последнее время посещение, и файлы, разбитые на блоки вместе с хешами, для дедупликации;
сами блоки файлов как большие бинарные объекты в отдельном файловом хранилище.
 

***Вывод:***

метаданные про пользователей и используемые файлы будет легче хранить в РСУБД с ACID из-за чувствительности (оплата подписки за сервис);
<br/>
с другой стороны, из-за большого количества блоков и скорости их прирастания, будет сложнее масштабироваться;
<br/>
можно рассмотреть колоночные БД с программной «ACID-изацией» критических участков системы, связанных с синхронизацией и т.д.;
<br/>
файловые блоки можем хранить как объекты в отдельном хранилище типа AmazonS3, повышая лимиты при необходимости.


**Приложение для выкладывания фотографий**

***Зачем нужен такой сервис?***
<br/>
Чтобы делиться фотографиями в зеркале и с едой с друзьями.


***Функциональные требования***

1. Пользователи могут загружать, скачивать и просматривать фотографии;
2. Пользователи могут искать другие фотографии по описанию;
3. Пользователи могут подписываться на фотографии друг друга;
4. Есть лента из популярных фото всех отслеживаемых пользователей.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пользователи перейдут в Snapchat или TikTok;
2. Лента с фотографиями должна отображаться за сотни миллисекунд (иначе см п. 1);
3. Консистентность не так критична — подписчики сколько-то потерпят без ваших селфи;
4. Высокая надёжность сервиса — ни одно селфи или фото еды не должно потеряться.


***Вводные данные***

***пользователей всего*** = 1 миллиард
<br/>
***DAU*** = 100 миллионов 
<br/>
***Загрузка фото в день*** = 1 (100 KB)
<br/>
Read / insert = 100 : 1


***Вычисления***

***Cеть***
<br/>
***Трафик*** 
<br/>
Gbps (Insert) = 1000 * 100 Kb ≈ 1 Gbps 
<br/>
Gbps (Read) = 1 Gbps * 100 ≈ 100 Gbps 
<br/>
Кол-во соединений = 100 Gbps / 100 KB = 125000


***Вычислительная нагрузка***
<br/>
RPS (insert) = 100 миллионов / 86400 (сек в день) ≈ 1000
<br/>
RPS (read) = 1000 * 100 = 100 000





***Хранилище***

***На горизонте 5 лет (Insert)*** = 5 x 365 x 1Gbps * 86400 (сек в день) = 20 PB
<br/>
***На горизонте 5 лет (Read)*** = 5 x 365 x 100Gbps * 86400 (сек в день) = 2 EB

***Cтоимость***

2 EB трафика обойдётся в $200M = 2e+9 GB * 0,1$ = 200 000 000
<br/>
на хранение 20 PB уйдёт $600k  = 20000 TB * 30$ = 600 000

***Вывод***

На датаноды понадобятся до сотни серверов, на соединения с трафиком и чтение БД ещё десятки. Поддержка всей системы обойдётся в сотни миллионов долларов на горизонте пяти лет.


***Первоочередные особенности***

1. Основных сценариев использования приложения два — загрузка или просмотры фотографий;
2. Фотографии занимают несравнимо больше, чем метаданные о снимках, постах и самих пользователях;
3. Можно отразить оба сценария в виде микро-приложений и необходимость разделения хранилища.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/photo.png)

***Какие данные храним:***

стандартный профиль пользователя (имя, почта, аватарка, даты создания и последнего посещения);
<br/>
фотографии (медиафайлы) и метаданные для них (id автора, теги, геометка, дата публикации и т.п.);
<br/>
таблицы подписок на посты пользователей.
 

***Вывод:***

т.к. для создания ленты пользователей (и рекомендаций) нужны JOIN-ы, очевидным выбором будет РСУБД типа Postgres, MyQSLи т.д.;
<br/>
но популярные сервисы с фотографиями так или иначе придётся масштабировать, и с РСУБД это делать сложнее;
<br/>
можно рассмотреть сценарий с «программными» JOIN-ами, когда все метаданные постов хранятся в колоночной БД типа Cassandra;
<br/>
ленту пользователя с рекомендациями и постами друзей можно также предпосчитать программно, и кешировать в key-value хранилищах типа Redis.

**Телеграм**

***Зачем нужен такой сервис?***
<br/>
Чтобы от общения вас отвлекали с работы, где не купили Slack.

***Функциональные требования***

1. Можно общаться один на один в чатах с другими пользователями;
2. Можно понять, онлайн ли пользователь и когда был последний раз;
3. Сервис обеспечивает постоянное хранение всей истории переписки;
4. Cтикеры, групповые чаты, кружочки с видео, беседы как в Clubhouse, стена, истории.

***Нефункциональные требования***

1. Общение происходит в реальном времени с минимальными задержками;
2. Консистентность — одинаковые сообщения в чатах на всех устройствах;
3. Высокая доступность сервиса — иначе пользователи перейдут в там-там.

***Вводные данные***

***DAU*** = 100 миллионов 
<br/>
***Сообщение на пользователя (insert)*** = 100 сообщений средним размером 1 KB;
<br/>
***Read*** = Каждое сообщение читает в среднем 10 человек (личные + группы).


***Вычисления***

***Вычислительная нагрузка***
<br/>
RPS (insert) = (100 миллионов * 100) / 84600 (сек в день) ≈ 100 000   
<br/>
RPS (read) = 100 000  * 10 = 1 000 000


***Cеть***
<br/>
***Трафик*** 
<br/>
Gbps (Insert) = 100 000 (RPS insert) * 1 Kb = 1  Gbps 
<br/>
Gbps (Read) = 1 000 000 (RPS read) * 1 Kb = 8  Gbps 
<br/>
Кол-во соединений = 8Gb (1 000 000 KB)  / 1 KB = 1 000 000


***Хранилище***
<br/>
***На горизонте 5 лет (Insert)*** = 5 x 365 x 1 Gb * 86400 (сек в день) ≈ 20 PB
<br/>
***На горизонте 5 лет (Read)*** = 5 x 365 x 8 Gbps * 86400 (сек в день) ≈ 200 PB


***Cтоимость***

200 PB трафика обойдётся в $20M = 2e+8 GB * 0,1$ = 20 000 000
<br/>
на хранение 20 PB уйдёт $600k  = 20000 TB * 30$ = 600 000

***Вывод***

В итоге нам понадобятся сотни серверов и для датанод для удержания соединений и для записей в БД. Поддержка всей системы обойдётся в десятки миллионов долларов на горизонте пяти лет.

***Первоочередные особенности***

1. Пользователь может обращаться либо к истории сообщений, либо вести беседы;
2. При беседе двух пользователей необходимо быстро доставлять их сообщения друг другу;
3. Можно отразить необходимость взаимодействия серверов, к которым обращаются пользователи.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/tg.png)



***Какие данные храним:***

онлайн данные об установленных соединениях, открытых чатах, каналах и т.п.;
<br/>
данные о пользователе (профиль, аватарки, время посещения, настройки приложения и т.п.);
<br/>
история чатов, изображения и прочие прикреплённые медиа и просто файлы.
 

***Вывод:***

для онлайн данных нужно по ключам user id получать и изменять сведения о соединениях, для чего подойдёт key-value БД типа Redis;
<br/>
данные о пользователях и их связях (скорее всего «контактах») можно хранить в реляционных БД вроде Postgres, а аватарки в S3-подобных хранилищах;
<br/>
т.к. количество сообщений будет большим, всю историю сообщений можно хранить в колоночных БД, а недавние сообщения выкладывать заранее в Redis.


**Твиттер**

***Зачем нужен такой сервис?***
<br/>
Чтобы делиться короткими сообщениями со всеми.

***Функциональные требования***

1. Пользователи могут добавлять короткие посты;
2. Посты могут содержать фото или видео;
3. Пользователи могут отслеживать посты других;
4. Есть лента из твитов всех отслеживаемых пользователей;
5. Бонус: поиск, реплаи, горячие темы, уведомления, рекомендации, аватарки с NFT обезьянами.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пользователи вернутся в живой журнал;
2. Лента с твитами должна отображаться за сотни миллисекунд (иначе см п. 1);
3. Консистентность не так критична — подписчики сколько-то потерпят без ваших мыслей.


***Вводные данные***

***DAU*** = 100 миллионов 
<br/>
***Insert***= 1 твит 
<br/>
***Read*** =  100 
<br/>
***Вес 1 твита***= = 10KB



***Вычисления***

***Вычислительная нагрузка***
<br/>
***RPS Insert*** = 100 миллионов * 1 / 84600 (сек в день)  ≈ 1000
<br/>
***RPS Read*** = 1000 * 100  ≈ 100 000

***Cеть***
<br/>
***Трафик*** 
<br/>
***Gbps (Insert)*** = 1000 (RPS insert) * 10 Kb ≈ 100 Mbps
<br/>
***Gbps (Read)*** = 100 000 (RPS read) * 10 Kb = 1 Gbps
<br/>
***Кол-во соединений*** = 1Gb (125000 KB) / 1 KB = 12 500

***Хранилище***
<br/>
***На горизонте 5 лет (Insert)*** = 5 x 365 x 100 Mbps * 86400 (сек в день) ≈ 2 PB
<br/>
***На горизонте 5 лет (Read)*** = 5 x 365 x 1 Gb * 86400 (сек в день) ≈ 20 PB

***Стоимость***
<br/>
200 PB трафика обойдётся в $20M, на хранение 2 PB уйдёт $60k.

***Вывод***
В итоге нам понадобятся десяток серверов для датанод + сервера вычислений и вспомогательные сервера. Поддержка всей системы обойдётся в десятки миллионов долларов на горизонте пяти лет.

***Первоочередные особенности***

1. Огромное количество пользователей, а значит, необходимость масштабирования;
2. Посты могут содержать тяжелые фото или видео, которые лучше хранить отдельно от данных про пользователей;
3. Можно сразу отметить наличие балансировщика нагрузки, а также наличие разных типов хранилищ.


![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/twit.png)


***Какие данные храним:***

стандартный профиль пользователя (имя, почта, аватарка, дата создания и последнего посещения);
<br/>
текст твита, медиафайлы + метаданные (id автора, теги, геометка, даты, лайки и т.п.);
<br/>
таблицы подписок и избранных постов.
 

***Вывод:***

в целом, требования очень похожи на фотоаппы – профили, подписки и лайки можно хранить в Postgres, а твиты в HBase + Redis
<br/>
ленту пользователя можно также предпосчитать и кешировать опять же в key-value хранилище типа Redis

**Нетфликс**

***Зачем нужен такой сервис?***
<br/>
Чтобы смотреть любимые фильмы, сериалы и передачи.

***Функциональные требования***

1. Пользователи могут просматривать фильмы и сериалы;
2. Пользователи могут искать контент по названию;
3. Сервис отслеживает оценки и статистику просмотров;
4. Пользователям рекомендуются новые фильмы и сериалы;
5. Бонус: жанры, популярное, избранное, списки на посмотреть потом.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пользователи будут смотреть котиков у конкурентов;
2. Высокая отзывчивость сервиса — видео должны проигрываться без подвисаний;
3. Консистентность не так критична — после недельного ожидания одна минута не заметна.


***DAU*** = 10 миллионов  
<br/>
***фильмов в библиотеке*** = 20000 
<br/>
***Каждый пользователь смотрит пару серий или фильм в течение 1 часа***
<br/>
***Видео*** = 1080p с битрейтом 10 Mbps

***Сеть*** 
<br/>
***трафик*** = 10M x 3600 (сек/час) x 10 Mbps = 40 PB (40 000 000 GB) в день!

***Вычисления***
10M x 10 (серий) / 84600 (сек в день) = 1k RPS на получение метаданных между сериями.


***Хранилище***
<br/>
20000 наименований по 10 часов (фильмы, но и сериалы) 20k x 10 x 3600 (сек/час)  x (50 + 10 + 5 + …) Mbps (версии разрешений)~= 5 PB.

***Стоимость***
<br/>
На хранилище нужно потратить десятки тысяч долларов, а на трафик миллиарды.

За трафик по стандартному прайсу расплатиться просто невозможно, поэтому этого и не происходит , клиент качает трафик с сервера своего провайдера:)



***Первоочередные особенности***

1. Пользователи могут просматривать как информация о фильмах, так и несравнимо более тяжёлые фильмы как таковые;
2. Пользователи могут смотреть фильмы с разных устройств, поэтому нужно подготовить версии фильмов разного размера;
3. Можно отметить использование нескольких типов хранилищ и необходимость подготовки фильмов под разные форматы.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/net.png)


***Какие данные храним:***

стандартные профили пользователей, справочную информацию о фильмах, оценки и добавление в избранное;
<br/>
разнообразная статистика по просмотрам для аналитики и построения рекомендаций;
<br/>
сами фильмы в разных разрешениях, звуковые дорожки и субтитры к ним.

***Вывод:***

для хранения стандартных табличных данных можно использовать РСУБД типа Postgres, частые фильмы положить в кеш (Redis);
<br/>
для аналитики и рекомендаций нужно будет собирать много логов в колоночную БД типа Cassandra;
<br/>
готовые рекомендации — в Redis. Сами видеофайлы и сопроводительные материалы нужно будет хранить в хранилищах типа S3, желательно геораспределённых.

**Поиск ресторанов**

***Зачем нужен такой сервис?***
<br/>
Чтобы поесть не холодную еду из зелёных и жёлтых сумок.

***Функциональные требования***

1. Пользователи могут добавлять/обновлять/удалять заведения;
2. Пользователи могут находить подходящие заведения поблизости;
3. Пользователи могут оставлять отзывы к посещённым местам с фото и оценками.
4. Бонус: фильтры по категориям, рекомендации новых заведений.

***Нефункциональные требования***

1. Поиск должен происходить быстро, иначе пользователь закажет доставку;
2. Гораздо большая нагрузка ожидается на подсервис поиска, чем на редактирование.



**Сервис такси**

***Зачем нужен такой сервис?***
<br/>
Чтобы легко доехать из точки А в точку Б с водителем.

***Функциональные требования***

1. Водители могут выходить на смену и ждать заказ;
2. Пассажиры могут находить водителей поблизости;
3. Пассажиры могут заказывать поездку, для неё находится водитель;
4. Позиции отслеживаются с момента принятия поездки и до её окончания. В конце поездки водитель продолжает смену;
5. Бонус: оценки водителям, оценки пассажирам, объяснение ценообразования.

***Нефункциональные требования***

1. Высокая доступность сервиса — иначе пассажиры поедут на метро, а водители на вокзал;
2. Низкое время ожидания для пассажира, маленький простой для водителя.


***Первоочередные особенности***

1. Есть фактически отдельные сервисы для водителя и пассажира как пользователей сервиса;
2. Нужно как обрабатывать текущую поездку, так и хранить исторические вместе с информацией о пользователях;
3. Можно отразить раздельность сервисов, необходимость ведения поездок и хранения истории.

![Image alt](https://github.com/dmatwe/projects/blob/main/System_design/ФТ%20НФТ%20Сервисов/png/taxi.png)

***Какие данные храним:***

профили пользователей, настройки приложений;
<br/>
онлайн данные о позициях пользователей и текущих поездках;
<br/>
офлайн данные об истории поездок, чат с поддержкой.

***Вывод:***

стандартные табличные данные с профилями и настройками — подойдёт РСУБД типа Postgres;
<br/>
данные о подключённых пользователях и текущих поездках храним в key-value БД, позиции водителей в специализированной гео-БД;
<br/>
при окончании поездки нуждающиеся в ACID обновления делаем в Postgres и кладём поездку в Cassandra, а недавние храним в Redis.